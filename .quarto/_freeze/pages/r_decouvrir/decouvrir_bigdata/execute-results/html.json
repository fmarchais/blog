{
  "hash": "630e46d824c35334932f5dd3fef464a2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"R pour le Big Data\"\nformat: html\ndescription: \"Traiter des données plus volumineuses que la mémoire vive avec R\"\nauthor: \"Félix Marchais\"\ndate: \"2025-07-30\"\ncategories:\n  - R\n  - Big Data\nexecute: \n  eval: false \n  echo: true\ndraft: true\n---\n\n\n\n\n\n# Big Data  \n\n## Apache Arrow et DuckDB  \n\nGrâce à Apache Arrow et Duckdb, il est possible de traiter de grands volumes de données facilement avec R. Ci-dessous, un exemple de requête sur un jeu de données de plus d'1 milliard de lignes. \\n\n\nLes données sont celles du dataset TLC Trip Record Data (https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page), et contiennent les informations de trajets de taxis new-yorkais. \\n\n\nDans cet exemple, les données de chaque mois entre janvier 2013 et avril 2025 ont été récupérées et regroupées dans un dossier appelé taxi_dataset. Ce dossier contient 147 fichiers au format .Parquet et pèse 14Go pour plus d'un milliard de lignes (le format .Parquet est un format compressé, le même jeu de données en .csv pèse environ 100Go). \\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) # Tidyverse sert à la manipulation de données\nlibrary(duckdb) # duckdb pour créer une base duckdb virtuelle\nlibrary(arrow) # Arrow pour lire des fichiers .Parquet\nlibrary(duckplyr) # Duckplyr pour utiliser dplyr avec le moteur duckdb\nlibrary(DBI) # DBI pour créer une connection à la base duckdb virtuelle\nlibrary(microbenchmark) # Pour vérifier les temps d'exécution\n\n# Créer une base duckdb virtuelle\nconn_ddb <- dbConnect(duckdb(), dbdir = \":memory:\")\n\nyellow_taxi <- conn_ddb %>% \n  tbl(\"read_parquet('D:\\\\taxi_dataset\\\\*.parquet')\" )\n# Il suffit d'indiquer le chemin vers le dossier contenant les 157 fichiers pour que {arrow} reconstitue le tout et le considère comme un seul dataset (il faut évidemment que les jeux de données suivent le même schéma)\n# Ici, yellow_taxi ne contient pas directement les données, mais une connexion au dataset.\n# on peut désormais requêter yellow_taxi comme un dataframe classique avec la grammaire de {dplyr}, sans avoir chargé les données dans la mémoire vive de R\n\nyellow %>% count()\n# 1.101.292.583 => 1 milliard de lignes\n\n# Utiliser microbenchmark() pour calculer le temps d'éxécution\nmicrobenchmark(\n  res = x <- yellow %>% \n    # filtrer les lignes\n    filter(trip_distance >=2) %>%  \n    # Créer une colonne \"year\" qui contient l'année du voyage\n    mutate(year = year(tpep_pickup_datetime)) %>% \n    # Aggréger tip_amount pour obtenir la moyenne du pourboire par nombre de passagers\n    summarize(mean_tip_amount = mean(tip_amount), .by = year) %>%\n    collect(), # Ajouter collect() pour récupérer les données\n  times = 1\n)\n# 14.2 secondes\n```\n:::\n\n\n\n\n\n\n## Spark et Hadoop  \n\nOn peut aussi se connecter à un cluster Spark, et envoyer ses requêtes depuis R avec la syntaxe de {dplyr} en utilisant {sparklyr}.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}