{
  "hash": "60e8422cc3cec96c2190777852fb13b7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Recueil de donn√©es\"\ndescription: \"Se connecter √† tout type de donn√©es\"\nformat: html\nauthor: \"F√©lix Marchais\"\ndate: \"2025-07-29\"\ncategories:\n  - R\n  - Bases de donn√©es\nexecute: \n  eval: false \n  echo: true\n  message: false\n  warning: false\n---\n\n\n\n\nR est un langage qui permet d'int√©ragir avec √† peu pr√®s n'importe quel format de donn√©es : \\n\n\n-   Des fichiers tabulaires (CSV, Excel, Parquet...), en local ou sur un serveur\n\n-   Des bases SQL/NoSQL : MS SQL Server, MySQL, MongoDB, Oracle, DuckDB...\n\n-   Des donn√©es h√©berg√©es sur un cloud (GCP, AWS, Azure...)\n\n-   Des donn√©es r√©cup√©r√©es via du webscraping ou une requ√™te API\n\n-   Des donn√©es au format JSON ou XML\n\nMais pas seulement ! R peut √©galement travailler avec des images (OCRiser un PDF pour r√©cup√©rer son contenu, ou en g√©n√©rer un), des fichiers DICOM, et bien plus encore.\n\n# CSV en local\n\nCommen√ßons avec le plus simple : les fichiers CSV\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr) # package du Tidyverse\nread_csv(\"path/to/iris.csv\")\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n```\n\n\n:::\n:::\n\n\n\n\n\nOn fait difficilement plus facile. On notera tout de m√™me que `{readr}` offre des arguments et fonctions suppl√©mentaires pour g√©rer diff√©rents probl√®mes que l'on rencontre souvent avec le CSV : les s√©parateurs et l'encodage. \\n\n\nLa fonction `read_csv` lit par d√©faut des fichiers au \"format international\" dont le s√©parateur est une virgule (,) et la d√©cimale un point (.). Cependant, en France (et en Europe), on utilis√© g√©n√©ralement la virgule comme s√©parateur d√©cimal. On a donc invent√© le format csv avec s√©parateur point-virgule (;) et d√©cimale en virgule (,). Pour lire un fichier sous ce format, on peut utiliser la fonction `read_csv2()` de `{readr}`. \\n\n\nPour des formats encore plus exotiques (tsv par exemple), `read_delim()` permet de pr√©ciser le d√©limiteur. \\n\n\nOn notera aussi que les fonctions `read_*()` fournissent des arguments permettant d'expliciter les valeurs nulles, de retirer les espaces en d√©but/fin de cha√Æne (ce qui arrive tr√®s souvent sur des donn√©es saisies √† la main dans Excel), de diff√©rencier des noms de colonnes en doublons, ou encore de sauter les premi√®res lignes d'un fichier.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\nread_csv2(\"path/to/file_fr.csv\", # lire un fichier au format FR, delim = ;\n          na = c(\"\",\"NULL\",\"NA\"), # les cases contenant \"\",\"NULL\" et \"NA\" seront vides\n          trim_ws = TRUE, # trim_whitespace : retirer les espaces en fin de cha√Æne\n          skip = 1, # sauter la premi√®re ligne\n          name_repair = \"unique\" # diff√©rencier les noms en doublons\n)\n```\n:::\n\n\n\n# CSV √† distance\n\nLes fonctions `read.*()` (en base R) et `read_*()` (`{readr}`) permettent toutes de charger des fichiers CSV via une URL.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncovid_data <- readr::read_csv2(\"https://www.data.gouv.fr/api/1/datasets/r/fe3e7099-a975-4181-9fb5-2dd1b8f1b552\")\nhead(covid_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 √ó 9\n  fra   strate2 jour       PourAvec tx_indic_7J_DC tx_indic_7J_hosp\n  <chr>   <dbl> <date>        <dbl> <chr>          <chr>           \n1 FR          0 2020-03-07        0 0              0               \n2 FR          0 2020-03-07        1 <NA>           0               \n3 FR          0 2020-03-07        2 <NA>           0               \n4 FR          0 2020-03-08        0 0              0               \n5 FR          0 2020-03-08        1 <NA>           0               \n6 FR          0 2020-03-08        2 <NA>           0               \n# ‚Ñπ 3 more variables: tx_indic_7J_SC <chr>, tx_prev_hosp <dbl>,\n#   tx_prev_SC <chr>\n```\n\n\n:::\n:::\n\n\n\n# Excel\n\nLes fichiers dits \"Excel\" ont l'extension .xls ou .xlsx, et peuvent contenir diff√©rents feuillets, de la mise en forme, des cellules fusionn√©es, des formules, des commentaires et des graphes. \\n\n\nPour lire ces donn√©es, il existe plusieurs librairies : `{readxl}`, `{openxslx}` ou `{openxslx2}`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readxl)\nread_excel(\"path/to/file.xslx\",\n           sheet = 1, # feuillet, par num√©ro ou par nom\n           range = \"B3:D87\", # les cellules √† garder (optionnel)\n           na = c(\"\",\"NULL\",\"NA\"), # les cellules √† consid√©rer vides\n           .name_repair = \"unique\" # diff√©rencier les noms en doublons\n)\n```\n:::\n\n\n\n# Parquet\n\nLe format .parquet, encore malheureusement peu connu, est un format compress√© et orient√© en colonnes, orient√© vers la performance. Un m√™me jeu de donn√©es au format parquet est entre 5 et 10 fois moins volumineux qu'en csv et est optimis√© pour √™tre lu rapidement par R ou Python. On citera aussi sa compatibilit√© avec DuckDB pour former un duo parfait pour [travailler le Big Data avec R](decouvrir_bigdata.qmd). L'un des rares d√©fauts qu'on peut lui trouver est de ne pas √™tre compatible avec Excel, ce qui limite sa diffusion. \\n\n\nSi vous ne l'avez pas encore lu, je vous renvoie vers cet excellent article : [Parquet devrait remplacer le format CSV](https://www.icem7.fr/cartographie/parquet-devrait-remplacer-le-format-csv/)\n\nIl existe deux m√©thodes principales pour lire un fichier parquet en R : `{arrow}`, la librairie de [Apache Arrow](https://arrow.apache.org/) et `{duckDB}`, la librairie de [DuckDB](https://duckdb.org/).\n\nLes deux m√©thodes permettent de cr√©er une *connexion* vers le jeu de donn√©es, au lieu de les importer dans la m√©moire de R, permettant de travailler avec des donn√©es pus volumineuses que la RAM. Les deux m√©thodes permettent d'utiliser `{dplyr}` pour le requ√™tage, mais `{duckdb}` permet aussi de requ√™ter n'importe quel fichier plat en SQL. \\n\n\nOn notera √©galement la capacit√© √† lire plusieurs fichiers regroup√©s dans un dossier (ayant le m√™me sch√©ma), voire m√™me des fichiers partitionn√©s ([au style Hive](https://arrow.apache.org/docs/r/reference/hive_partition.html)), permettant de ne scanner que les donn√©es n√©cessaires, pour optimiser encore plus la performance de requ√™tage.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Arrow : \nlibrary(arrow)\narrow_df <- read_parquet(\"path/to/file.parquet\")\n\n# Il est possible de lire directement un ensemble de fichiers .parquet ayant le m√™me sch√©ma \n# et regroup√©s dans un dossier (donn√©es par batch, avec un fichier par mois par exemple)\narrow_dataset <- open_dataset(\"path/to/folder\")\n# Ici, arrow_dataset est une connexion, de classe `Dataset`\n# Un `Dataset` de {arrow} se requ√™te avec {dplyr}\n# pour r√©cup√©rer les donn√©es dans l'environnement de R : \narrow_df <- arrow_dataset |> \n  filter(...) |>  # la plupart des fonctions de {dplyr} sont compatibles\n  select(...) |>  \n  collect() # pour ex√©cuter la requ√™te et r√©cup√©rer un tibble/dataframe\n\n\n\n# DuckDB :\nlibrary(duckdb)\nlibrary(DBI) # DataBase Interface\nlibrary(dplyr)\n\nconn_ddb <- DBI::dbConnect(duckdb(), dbdir = \":memory:\") # Cr√©er une base duckdb virtuelle\nduck_df <- conn_ddb |> \n  tbl(\"path/dataset/**/*.parquet\") |> \n  filter(...)\n# Le ** signifie \"tout\", et permet de lire tous les fichiers .parquet du dossier\n# Dans un fichier partitionn√© par exemple\n```\n:::\n\n\n\n\n:::{.callout-tip}\n## Note  \n{arrow} renvoie des objets de type `arrow_table` ou `Dataset`, qui ne sont compatibles qu'avec les fonctions de `{dplyr}`. Si vous souhaitez modifier une colonne avec une fonction de `{purrr}`, `{lubridate}` ou `{stringr}`, il faudra d'abord utiliser `collect()` pour obtenir un `tibble`. \\n\n\nA l'inverse, `{duckdb}` renvoie des objets de classe `tbl` (donc des `tibble`). Si l'on ajoute l'incroyable performance de requ√™tage du moteur duckdb, j'aurais plut√¥t tendance √† privil√©gier cette m√©thode\n:::\n\n:::{.callout-important}\n## Info  \nR√©cemment, `{duckplyr}`, une librairie int√©grant `{dplyr}` avec le moteur de `{duckdb}` [a rejoint le `Tidyverse`](https://www.tidyverse.org/blog/2025/06/duckplyr-1-1-0/), signe que Duckdb est per√ßu comme un outil d'avenir.\n\n:::\n\n\n# Connexions aux bases SQL\n\nSi vous utilisez Rstudio, le moyen le plus simple de se connecter √† une source de donn√©es est d'utiliser l'onglet *Connections*, g√©n√©ralement situ√© en haut √† droite, avec votre *Environnement*. En cliquant sur *New connection*, une fen√™tre appara√Æt et va automatiquement vous proposer les sources √† disposition. \\n\n\n![Exemple des connexions existantes sur mon poste](/images/rstudio_connections.png)\n\nSi vous avez d√©j√† configur√© un DSN contenant vos identifiants, les informations de connexion devraient d√©j√† √™tre remplies, vous n'avez plus qu'√† cliquer sur \"OK\", et le code s'ex√©cutera dans la console. Vous pourrez alors visualiser vos bases dans l'onglet *Connections*\n\n![](/images/rstudio_connections_dsn.png){width=\"515\"}\n\nSinon, vous aurez besoin d'entrer les param√®tres suivants :\n\n-   *user* = \"...\" pour le nom d'utilisateur\n\n-   *password* = \"...\" pour le mot de passe\n\n-   *host* et *port* si besoin\n\n-   *dbname* pour le nom de la base de donn√©es\n\nSi vous utilisez uniquement la console, vous pouvez rentrer directement les param√®tres dans `dbConnect()`, en arguments de la fonction.\\n\n\nUne fois connect√©, on peut requ√™ter ses donn√©es avec `{dplyr}` ou en SQL, en utilisant `dbGetQuery()`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DBI)\nlibrary(odbc)\nconn <- DBI::dbConnect(odbc::odbc() ,\"server-name\", database = \"MA_BASE\")\n\n# SQL\nma_table <- DBI::dbGetQuery(conn, \n                            \"SELECT * FROM MA_TABLE\n                            WHERE ...\")\n\n\n# DPLYR\nlibrary(dplyr)\nma_table_2 <- dplyr::tbl(conn, \"MA_TABLE\") |>  # ici, on cr√©√© une connexion √† la table\n  dplyr::filter(...) |>  # on cr√©√© la requ√™te\n  dplyr::collect() # et on collecte le r√©sultat dans R avec collect()\n```\n:::\n\n\n\nPour interagir avec une base SQL pour autre chose qu'une requ√™te (UPDATE, DROP, ...), on peut utiliser la librairie `{dbplyr}`, un back-end de `{dplyr}` pour les bases de donn√©es, ou utiliser `DBI::dbExecute()`\n\n# Cloud\n\nR dispose de nombreuses librairies pour r√©cup√©rer des donn√©es h√©berg√©es sur un serveur cloud. \\n Je n'entrerai pas dans les d√©tails de cette partie, car je n'ai pas encore eu beaucoup l'occasion de pratiquer par moi-m√™me, mais je penserai √† la mettre √† jour d√®s que possible. \\n\n\n## Google Cloud Platform\n\nVoici une liste non exhaustive des librairies permettant de travailler avec Google Cloud Platform, disponible sur la vignette de [`{googleAuthR}`](https://code.markedmondson.me/googleAuthR/)\n\n-   [googleComputeEngineR](https://cloudyr.github.io/googleComputeEngineR/) - Google Compute Engine VMs API\n\n-   [searchConsoleR](https://code.markedmondson.me/searchConsoleR/) - Search Console API\n\n-   [bigQueryR](https://code.markedmondson.me/bigQueryR/) - BigQuery API. Part of the cloudyr project.\n\n-   [googleAnalyticsR](https://code.markedmondson.me/googleAnalyticsR/) - Google Analytics API\n\n-   [googleTagManagerR](https://github.com/IronistM/googleTagManageR) - Google Tag Manager API by IronistM\n\n-   [googleID](https://github.com/MarkEdmondson1234/googleID) - Simple user info from G+ API for Shiny app authentication flows.\n\n-   [googleCloudStorageR](https://code.markedmondson.me/googleCloudStorageR/) - Google Cloud Storage API\n\n-   [RoogleVision](https://github.com/cloudyr/googleCloudVisionR) - R Package for Image Recogntion, Object Detection, and OCR using the Google‚Äôs Cloud Vision API\n\n-   [googleLanguageR](https://github.com/ropensci/googleLanguageR) - Access Speech to Text, Entity analysis and translation APIs from R\n\n-   [googleCloudRunner](https://code.markedmondson.me/googleCloudRunner/) - Continuous Development and Integration with Cloud Run, Cloud Scheduler and Cloud Build\n\n## AWS\n\nLa documentation officielle de Amazon Web Service dispose [d'un tutoriel](https://aws.amazon.com/fr/blogs/opensource/getting-started-with-r-on-amazon-web-services/) pour acc√©der aux donn√©es du service depuis R.\n\n# API REST\n\nLe plus simple selon moi pour requ√™ter une API REST avec R est d'utiliser l'excellent `{httr2}` de Hadley Wickham (que vous connaissez d√©j√† certainement si vous utilisez le Tidyverse). La documentation du package est disponible [ici](https://httr2.r-lib.org/)\n\nLa premi√®re chose √† faire est √©videmment de lire la documentation de l'API que l'on souhaite requ√™ter. Pour vous entra√Æner, vous pouvez utiliser [une des API disponibles sur data.gouv.fr](https://www.data.gouv.fr/dataservices). Pour cet exemple, j'utiliserai [l'API de Hub'eau pour la qualit√© de l'eau potable en France](https://hubeau.eaufrance.fr/page/api-qualite-eau-potable#console). \\n\n\nUne fois rendus sur le site, la documentation nous indique que la \"Base URL\" est hubeau.eaufrance.fr/api, puis nous indique les extensions √† ajouter selon ce que l'on souhaite requ√™ter (liste des communes ou r√©sultats), puis les parm√®tres de filtrage\n\nOn utilisera √©galement la librairie `{jsonlite}` pour convertir les donn√©es r√©cup√©r√©es au format JSON en un data.frame\n\n::: callout-tip\n## Astuce\nLe package `{httr2}` n√©cessite l'installation du package `{curl}`, qui peut poser probl√®me si vous √™tes sur un r√©seau professionnel prot√©g√© par un pare-feu. Dans ce cas, contactez votre DSI, ou ... faites un partage de connexion depuis votre t√©l√©phone ü§´\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(httr2)\nlibrary(jsonlite)\n\nbase_url <- \"http://hubeau.eaufrance.fr/api\"\n\nreq <- request(base_url) |>  # pointer vers l'url de base\n  req_url_path_append(\"/v1/qualite_eau_potable/resultats_dis\") |> #extension\n  req_url_query(code_departement = \"85\") |> # param√®tres de la requ√™te\n  req_url_query(size = \"50\") |> \n  req_url_query(    \n    fields = c('libelle_parametre','libelle_parametre_maj',\n               'resultat_numerique', 'libelle_unite',\n               'limite_qualite_parametre','reference_qualite_parametre',\n               'nom_commune','date_prelevement',\n               'conclusion_conformite_prelevement'), \n    .multi = \"comma\")\n\nresp <- req_perform(req)\n\ndf_data <- resp |>\n  resp_body_string() |>\n  fromJSON()\ndf_data <- df_data$data\n\nhead(df_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                                    libelle_parametre\n1                  Cel. de cyanobact√©ries toxinog√®nes\n2 Cyanobact√©ries toxinog√®nes (exprim√©es en biovolume)\n3           Num√©ration des cellules de phytoplanctons\n4                          Cellules de cyanobact√©ries\n5                              Microcystine-RR totale\n6          Somme des microcystines analys√©es (calcul)\n                       libelle_parametre_maj resultat_numerique  libelle_unite\n1         CEL. DE CYANOBACT√âRIES TOXINOG√àNES              0.041 n(cellules)/mL\n2                 CYANOBACT√âRIES TOXINOG√àNES              0.732          mm3/L\n3                 CELLULES DE PHYTOPLANCTONS              1.000 n(cellules)/mL\n4                 CELLULES DE CYANOBACT√âRIES              0.041 n(cellules)/mL\n5                     MICROCYSTINE-RR TOTALE              0.000           ¬µg/L\n6 SOMME DES MICROCYSTINES ANALYS√âES (CALCUL)              0.000           ¬µg/L\n  limite_qualite_parametre reference_qualite_parametre nom_commune\n1                     <NA>                        <NA>    APREMONT\n2                     <NA>                        <NA>    APREMONT\n3                     <NA>                        <NA>    APREMONT\n4                     <NA>                        <NA>    APREMONT\n5                 <=1 ¬µg/L                        <NA>    APREMONT\n6                 <=1 ¬µg/L                        <NA>    APREMONT\n      date_prelevement\n1 2025-06-30T14:00:00Z\n2 2025-06-30T14:00:00Z\n3 2025-06-30T14:00:00Z\n4 2025-06-30T14:00:00Z\n5 2025-06-30T13:55:00Z\n6 2025-06-30T13:55:00Z\n                                                                        conclusion_conformite_prelevement\n1 Eau d'alimentation conforme aux exigences de qualit√© en vigueur pour l'ensemble des param√®tres mesur√©s.\n2 Eau d'alimentation conforme aux exigences de qualit√© en vigueur pour l'ensemble des param√®tres mesur√©s.\n3 Eau d'alimentation conforme aux exigences de qualit√© en vigueur pour l'ensemble des param√®tres mesur√©s.\n4 Eau d'alimentation conforme aux exigences de qualit√© en vigueur pour l'ensemble des param√®tres mesur√©s.\n5 Eau d'alimentation conforme aux exigences de qualit√© en vigueur pour l'ensemble des param√®tres mesur√©s.\n6 Eau d'alimentation conforme aux exigences de qualit√© en vigueur pour l'ensemble des param√®tres mesur√©s.\n```\n\n\n:::\n:::\n\n\n\n::: callout-tip\n## Cacher les cl√©s API\nDans le cas o√π vous utilisez une API priv√©e n√©cessitant une cl√©, il est judicieux d'√©viter de stocker vos identifiants dans le script R. Pour cela, vous pouvez utiliser un fichier [.Rprofile](https://docs.posit.co/ide/user/ide/guide/environments/r/managing-r.html), stock√© sur votre machine locale, et qui se lance quand vous d√©marrez votre IDE.\nAinsi, vos identifiants sont pr√©sents dans votre environnement sans avoir √† les d√©clarer dans le script. Si vous h√©bergez votre code sur GitHub, vous pouvez inclure le .Rprofile dans le .gitignore pour √©viter de les inclure dans le repo, et utiliser la fonction [*Github Secrets*](https://docs.github.com/fr/actions/how-tos/write-workflows/choose-what-workflows-do/use-secrets) pour remplacer le .Rprofile.\n:::\n\n\n\n# Webscraping  \n\nSi vous souhaitez r√©cup√©rer des donn√©es sur un site web qui ne propose pas d'API, l'une des solutions est le [*web scraping*](https://fr.wikipedia.org/wiki/Web_scraping). \n\n\n::: callout-warning\n## Attention\nTous les sites n'autorisent pas le web scraping, et les donn√©es doivent √™tre *nativement* publiques, *tomb√©es* dans le domaine publique ou sous licence libre *si votre usage n'est pas commercial*. Renseignez-vous avant de scraper une page !\n:::\n\nLe principal package de web scraping en R est [`{rvest}`](https://rvest.tidyverse.org/index.html) (toujours d√©velopp√© par Hadley Wickham). \nPour cet exemple, j'utiliserai aussi [`{polite}`](https://dmi3kno.github.io/polite/index.html), un package qui permet de suivre 3 r√®gles d'√©thique lors d'une session de scraping : *\"Demander la permission, prendre doucement et ne jamais demander deux fois\"*.\nCes r√®gles permettent d'√©viter de causer des probl√®mes en r√©coltant des donn√©es non autoris√©es ou en surchargeant le serveur de requ√™tes, et nous √©viterons d'√™tre bannis par les administrateurs du site scrap√©.\n\nDans cet exemple, nous allons r√©cup√©rer une table contenant la liste des communes de Vend√©e, sur [cette page wikip√©dia](https://fr.wikipedia.org/wiki/Liste_des_communes_de_la_Vend%C3%A9e)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rvest)\nlibrary(polite)\n\nurl <- \"https://fr.wikipedia.org/wiki/Liste_des_communes_de_la_Vend%C3%A9e\"\nsession <- polite::bow(url) \n# Bow permet d'interroger le robots.txt et nous informe du r√©sultat\n# il enregistre notamment le d√©lai minimum de requ√™tage autoris√© par le site\n\nsession\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<polite session> https://fr.wikipedia.org/wiki/Liste_des_communes_de_la_Vend%C3%A9e\n    User-agent: polite R package\n    robots.txt: 261 rules are defined for 34 bots\n   Crawl delay: 5 sec\n  The path is scrapable for this user-agent\n```\n\n\n:::\n\n```{.r .cell-code}\npage <- polite::scrape(session)\n# scrape() r√©cup√®re le contenu autoris√©\n\ncities_table <- page |> \n  rvest::html_element(\"table.wikitable\") |> \n  rvest::html_table()\n\n# html_element r√©cup√®re le premier √©l√©ment de la classe \"table.wikitable\"\n# pour tous les r√©cup√©rer sous forme de liste : html_elements()\n# html_table() permet de convertir des donn√©es tabulaires en un dataframe\n\nhead(cities_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 √ó 10\n  Nom             CodeInsee `Code postal` Arrondissement Canton Intercommunalit√©\n  <chr>               <int>         <dbl> <chr>          <chr>  <chr>           \n1 La Roche-sur-Y‚Ä¶     85191         85000 La Roche-sur-‚Ä¶ La Ro‚Ä¶ CA La Roche-sur‚Ä¶\n2 Les Achards         85152         85150 Les Sables-d‚Äô‚Ä¶ Talmo‚Ä¶ Pays-des-Achards\n3 L'Aiguillon-la‚Ä¶     85001         85460 Les Sables-d'‚Ä¶ Mareu‚Ä¶ CC Sud-Vend√©e-L‚Ä¶\n4 L'Aiguillon-su‚Ä¶     85002         85220 Les Sables-d'‚Ä¶ Saint‚Ä¶ CA Pays de Sain‚Ä¶\n5 Aizenay             85003         85190 La Roche-sur-‚Ä¶ Aizen‚Ä¶ CC de Vie-et-Bo‚Ä¶\n6 Angles              85004         85750 Les Sables-d'‚Ä¶ Mareu‚Ä¶ CC Vend√©e-Grand‚Ä¶\n# ‚Ñπ 4 more variables: `Superficie(km2)` <chr>,\n#   `Population(derni√®re pop. l√©gale)` <chr>, `Densit√©(hab./km2)` <chr>,\n#   Modifier <lgl>\n```\n\n\n:::\n:::\n\n\n\n\n\n# OCR et PDF  \n\nDans le domaine de la sant√©, les compte-rendus m√©dicaux contiennent √©norm√©ment de donn√©es int√©ressantes, mais sont malheureusement difficilement exploitables car dans un format non-structur√© : un papier scann√© et stock√© en PDF.\n\nL'OCR (Optical Character Recognition) est un syst√®me de *machine learning* permettant d'extraire le texte pr√©sent sur une image, parfait donc pour exploiter un grand nombre de comtpe-rendus scann√©s sans avoir √† le faire manuellement.\n\nDans cet exemple, nous utiliserons le mod√®le **Tesseract**, d√©velopp√© par Google et aujourd'hui Open Source. Nous utiliserons √©galement la librairie `doParallel` pour parall√©liser le traitement des images et augmenter la vitesse d'OCRisation\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tesseract)\nlibrary(doParallel)\nlibrary(pdftools)\n\n# R√©cup√©rer le chemin de tous les fichiers PDF pr√©sents dans un dossier\nlist_pdf <- list.files(path = \"path/to/folder\", \n                       pattern = \"\\\\.pdf$\", \n                       all.files = TRUE, \n                       full.names = TRUE,\n                       recursive = FALSE\n                       )\n\nncores <- doParallel::detectCores(logical = FALSE) # compter les coeurs physiques \n\n# Cr√©er le cluster, en laissant 2 coeurs non utilis√©s\ncl <- doParallel::makeCluster(ncores-2)   \n# et pour arr√™ter : stopCluster(cl)\n\n\n# Charger les librariries et les objets dans les clusters\n## clusterEvalQ() : ex√©cute le code dans le cluster\nclusterEvalQ(cl, {\n  library(tidyverse);\n  library(tesseract);\n  library(pdftools)\n  })\n\n## clusterExport() charge l'objet dans le cluster\nclusterExport(cl, c(\"list_pdf\"))\n\n\n# Cr√©er une fonction qui renvoie un dataframe en sortie (1 ligne par page)\n\nmyPdfConvert <- function(list_objet) { \n  list_objet %>% map_df(~ data.frame(doc_origin = .x, #.x = chaque √©l√©ment de listobjet\n                                     texte = ocr(pdftools::pdf_convert(.x, dpi = 200)) # oc√©rise un pdf converti en png\n                                     ))\n}\n\n\n# Appliquer la fonction myPdfConvert √† chaque √©l√©ment de list_pdf\nres <- parLapply(cl, list_pdf, myPdfConvert)\n# Le r√©sultat est obtenu sous forme de liste (1 √©l√©ment par cluster) => les recombiner\nres <- do.call(\"rbind\", res)\n\n# Arr√™ter le cluster\nstopCluster(cl)\n\n\n# Pour r√©cup√©rer num√©ro de page et nom du doc √† chaque ligne (1 ligne = 1 page)\nresr <- res %>% \n  group_by(doc_origin) %>% \n  mutate(doc_page = 1:n(), doc_nom = str_extract_all(doc_origin, \"[[:alnum:]_ ]*\\\\.pdf\")) %>%\n  ungroup()\n```\n:::\n\n\n\n\nPour visualiser l'impact de la parall√©lisation : \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](decouvrir_connections_files/figure-html/ocr2-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n# JSON et XML  \n\nLes formats **XML** (eXtensible Markup Language) et **JSON** (JavaScript Object Notation) sont largement utilis√©s pour le stockage et l‚Äô√©change de donn√©es, notamment dans les **API REST**, les **flux RSS**, ou le stockage de donn√©es hi√©rarchiques .\n\nPour lire les fichiers XML, on utilisera`{xml2}` et pour JSON :`{jsonlite}` (il en existe de nombreuses autres, mais ce sont les plus connues)\n\n## XML  \n\nSupposons que nous ayons un fichier \"livres.xml\" ressemblant √† cela :\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(xml2)\n\nxml_doc <- as_xml_document(\n\"<livres>\n  <livre>\n    <titre>R pour les d√©butants</titre>\n    <auteur>Jean Martin</auteur>\n    <annee>2020</annee>\n  </livre>\n  <livre>\n    <titre>Analyse de donn√©es avec R</titre>\n    <auteur>Marie Dupont</auteur>\n    <annee>2021</annee>\n  </livre>\n</livres>\"\n)\n```\n:::\n\n\n\nPour lire le fichier .xml, on utilisera\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxml_doc <- xml2::read_xml(\"livres.xml\")\n```\n:::\n\n\n\nEnsuite, on peut extraire les titres avec\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitres_xml <- xml_find_all(xml_doc, \".//livre/titre\")\nprint(titres_xml)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{xml_nodeset (2)}\n[1] <titre>R pour les d√©butants</titre>\n[2] <titre>Analyse de donn√©es avec R</titre>\n```\n\n\n:::\n:::\n\n\n\nLes fonctions `xml_find_*()` permettent d'utiliser des expression **xpath**, similaire √† du **REGEX** mais pour les architectures en arbre et renvoient des objets de classe `xml_nodeset`. Pour r√©cup√©rer un vecteur `character`, on rajoute simplement `xml_text()`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitres <- xml_text(titres_xml)\nprint(titres)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"R pour les d√©butants\"      \"Analyse de donn√©es avec R\"\n```\n\n\n:::\n:::\n\n\n\n\n## JSON  \n\nPour lire du JSON, on utilisera simplement `fromJSON()`, pour convertir une cha√Æne de texte, ou `read_json()` pour lire un fichier .json. Les deux fonctions renvoient directement un `data.frame`\n\n\n\n\n```{.r .cell-code}\nlibrary(jsonlite)\n\nlivres_json <- fromJSON(\n'\n[\n  {\n    \"titre\": \"R pour les d√©butants\",\n    \"auteur\": \"Jean Dupont\",\n    \"annee\": 2020\n  },\n  {\n    \"titre\": \"Analyse de donn√©es avec R\",\n    \"auteur\": \"Marie Curie\",\n    \"annee\": 2021\n  }\n]\n')\n\nprint(livres_json)\n```\n\n                      titre      auteur annee\n1      R pour les d√©butants Jean Dupont  2020\n2 Analyse de donn√©es avec R Marie Curie  2021\n\n\n\nOn citera aussi la possibilit√© de convertir un objet R en JSON avec la fonction `toJSON()`. \n\n\n\n# DICOM  \n\nSans rentrer dans les d√©tails ici, le format DICOM (Digital Imaging and Communications in Medicine) est un standard pour l'imagerie m√©dicale. le package `{Espadon}` [d√©velopp√© par le CNRS](https://espadon.cnrs.fr/) permet d'interagir avec ce format\n\n# GPX  \n\nGPX est un format pour les donn√©es g√©ospatiales, qui permet par exemple de cr√©er un itin√©raire sur une carte. [Cet article](https://www.appsilon.com/post/r-gpx-files) explique tr√®s bien comment lire ce standard, souvent stock√© au format XML.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "decouvrir_connections_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}