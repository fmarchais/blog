{
  "hash": "60e8422cc3cec96c2190777852fb13b7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Recueil de données\"\ndescription: \"Se connecter à tout type de données\"\nformat: html\nauthor: \"Félix Marchais\"\ndate: \"2025-07-29\"\ncategories:\n  - R\n  - Bases de données\nexecute: \n  eval: false \n  echo: true\n  message: false\n  warning: false\n---\n\n\n\n\nR est un langage qui permet d'intéragir avec à peu près n'importe quel format de données : \\n\n\n-   Des fichiers tabulaires (CSV, Excel, Parquet...), en local ou sur un serveur\n\n-   Des bases SQL/NoSQL : MS SQL Server, MySQL, MongoDB, Oracle, DuckDB...\n\n-   Des données hébergées sur un cloud (GCP, AWS, Azure...)\n\n-   Des données récupérées via du webscraping ou une requête API\n\n-   Des données au format JSON ou XML\n\nMais pas seulement ! R peut également travailler avec des images (OCRiser un PDF pour récupérer son contenu, ou en générer un), des fichiers DICOM, et bien plus encore.\n\n# CSV en local\n\nCommençons avec le plus simple : les fichiers CSV\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr) # package du Tidyverse\nread_csv(\"path/to/iris.csv\")\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n```\n\n\n:::\n:::\n\n\n\n\n\nOn fait difficilement plus facile. On notera tout de même que `{readr}` offre des arguments et fonctions supplémentaires pour gérer différents problèmes que l'on rencontre souvent avec le CSV : les séparateurs et l'encodage. \\n\n\nLa fonction `read_csv` lit par défaut des fichiers au \"format international\" dont le séparateur est une virgule (,) et la décimale un point (.). Cependant, en France (et en Europe), on utilisé généralement la virgule comme séparateur décimal. On a donc inventé le format csv avec séparateur point-virgule (;) et décimale en virgule (,). Pour lire un fichier sous ce format, on peut utiliser la fonction `read_csv2()` de `{readr}`. \\n\n\nPour des formats encore plus exotiques (tsv par exemple), `read_delim()` permet de préciser le délimiteur. \\n\n\nOn notera aussi que les fonctions `read_*()` fournissent des arguments permettant d'expliciter les valeurs nulles, de retirer les espaces en début/fin de chaîne (ce qui arrive très souvent sur des données saisies à la main dans Excel), de différencier des noms de colonnes en doublons, ou encore de sauter les premières lignes d'un fichier.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\nread_csv2(\"path/to/file_fr.csv\", # lire un fichier au format FR, delim = ;\n          na = c(\"\",\"NULL\",\"NA\"), # les cases contenant \"\",\"NULL\" et \"NA\" seront vides\n          trim_ws = TRUE, # trim_whitespace : retirer les espaces en fin de chaîne\n          skip = 1, # sauter la première ligne\n          name_repair = \"unique\" # différencier les noms en doublons\n)\n```\n:::\n\n\n\n# CSV à distance\n\nLes fonctions `read.*()` (en base R) et `read_*()` (`{readr}`) permettent toutes de charger des fichiers CSV via une URL.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncovid_data <- readr::read_csv2(\"https://www.data.gouv.fr/api/1/datasets/r/fe3e7099-a975-4181-9fb5-2dd1b8f1b552\")\nhead(covid_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 9\n  fra   strate2 jour       PourAvec tx_indic_7J_DC tx_indic_7J_hosp\n  <chr>   <dbl> <date>        <dbl> <chr>          <chr>           \n1 FR          0 2020-03-07        0 0              0               \n2 FR          0 2020-03-07        1 <NA>           0               \n3 FR          0 2020-03-07        2 <NA>           0               \n4 FR          0 2020-03-08        0 0              0               \n5 FR          0 2020-03-08        1 <NA>           0               \n6 FR          0 2020-03-08        2 <NA>           0               \n# ℹ 3 more variables: tx_indic_7J_SC <chr>, tx_prev_hosp <dbl>,\n#   tx_prev_SC <chr>\n```\n\n\n:::\n:::\n\n\n\n# Excel\n\nLes fichiers dits \"Excel\" ont l'extension .xls ou .xlsx, et peuvent contenir différents feuillets, de la mise en forme, des cellules fusionnées, des formules, des commentaires et des graphes. \\n\n\nPour lire ces données, il existe plusieurs librairies : `{readxl}`, `{openxslx}` ou `{openxslx2}`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readxl)\nread_excel(\"path/to/file.xslx\",\n           sheet = 1, # feuillet, par numéro ou par nom\n           range = \"B3:D87\", # les cellules à garder (optionnel)\n           na = c(\"\",\"NULL\",\"NA\"), # les cellules à considérer vides\n           .name_repair = \"unique\" # différencier les noms en doublons\n)\n```\n:::\n\n\n\n# Parquet\n\nLe format .parquet, encore malheureusement peu connu, est un format compressé et orienté en colonnes, orienté vers la performance. Un même jeu de données au format parquet est entre 5 et 10 fois moins volumineux qu'en csv et est optimisé pour être lu rapidement par R ou Python. On citera aussi sa compatibilité avec DuckDB pour former un duo parfait pour [travailler le Big Data avec R](decouvrir_bigdata.qmd). L'un des rares défauts qu'on peut lui trouver est de ne pas être compatible avec Excel, ce qui limite sa diffusion. \\n\n\nSi vous ne l'avez pas encore lu, je vous renvoie vers cet excellent article : [Parquet devrait remplacer le format CSV](https://www.icem7.fr/cartographie/parquet-devrait-remplacer-le-format-csv/)\n\nIl existe deux méthodes principales pour lire un fichier parquet en R : `{arrow}`, la librairie de [Apache Arrow](https://arrow.apache.org/) et `{duckDB}`, la librairie de [DuckDB](https://duckdb.org/).\n\nLes deux méthodes permettent de créer une *connexion* vers le jeu de données, au lieu de les importer dans la mémoire de R, permettant de travailler avec des données pus volumineuses que la RAM. Les deux méthodes permettent d'utiliser `{dplyr}` pour le requêtage, mais `{duckdb}` permet aussi de requêter n'importe quel fichier plat en SQL. \\n\n\nOn notera également la capacité à lire plusieurs fichiers regroupés dans un dossier (ayant le même schéma), voire même des fichiers partitionnés ([au style Hive](https://arrow.apache.org/docs/r/reference/hive_partition.html)), permettant de ne scanner que les données nécessaires, pour optimiser encore plus la performance de requêtage.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Arrow : \nlibrary(arrow)\narrow_df <- read_parquet(\"path/to/file.parquet\")\n\n# Il est possible de lire directement un ensemble de fichiers .parquet ayant le même schéma \n# et regroupés dans un dossier (données par batch, avec un fichier par mois par exemple)\narrow_dataset <- open_dataset(\"path/to/folder\")\n# Ici, arrow_dataset est une connexion, de classe `Dataset`\n# Un `Dataset` de {arrow} se requête avec {dplyr}\n# pour récupérer les données dans l'environnement de R : \narrow_df <- arrow_dataset |> \n  filter(...) |>  # la plupart des fonctions de {dplyr} sont compatibles\n  select(...) |>  \n  collect() # pour exécuter la requête et récupérer un tibble/dataframe\n\n\n\n# DuckDB :\nlibrary(duckdb)\nlibrary(DBI) # DataBase Interface\nlibrary(dplyr)\n\nconn_ddb <- DBI::dbConnect(duckdb(), dbdir = \":memory:\") # Créer une base duckdb virtuelle\nduck_df <- conn_ddb |> \n  tbl(\"path/dataset/**/*.parquet\") |> \n  filter(...)\n# Le ** signifie \"tout\", et permet de lire tous les fichiers .parquet du dossier\n# Dans un fichier partitionné par exemple\n```\n:::\n\n\n\n\n:::{.callout-tip}\n## Note  \n{arrow} renvoie des objets de type `arrow_table` ou `Dataset`, qui ne sont compatibles qu'avec les fonctions de `{dplyr}`. Si vous souhaitez modifier une colonne avec une fonction de `{purrr}`, `{lubridate}` ou `{stringr}`, il faudra d'abord utiliser `collect()` pour obtenir un `tibble`. \\n\n\nA l'inverse, `{duckdb}` renvoie des objets de classe `tbl` (donc des `tibble`). Si l'on ajoute l'incroyable performance de requêtage du moteur duckdb, j'aurais plutôt tendance à privilégier cette méthode\n:::\n\n:::{.callout-important}\n## Info  \nRécemment, `{duckplyr}`, une librairie intégrant `{dplyr}` avec le moteur de `{duckdb}` [a rejoint le `Tidyverse`](https://www.tidyverse.org/blog/2025/06/duckplyr-1-1-0/), signe que Duckdb est perçu comme un outil d'avenir.\n\n:::\n\n\n# Connexions aux bases SQL\n\nSi vous utilisez Rstudio, le moyen le plus simple de se connecter à une source de données est d'utiliser l'onglet *Connections*, généralement situé en haut à droite, avec votre *Environnement*. En cliquant sur *New connection*, une fenêtre apparaît et va automatiquement vous proposer les sources à disposition. \\n\n\n![Exemple des connexions existantes sur mon poste](/images/rstudio_connections.png)\n\nSi vous avez déjà configuré un DSN contenant vos identifiants, les informations de connexion devraient déjà être remplies, vous n'avez plus qu'à cliquer sur \"OK\", et le code s'exécutera dans la console. Vous pourrez alors visualiser vos bases dans l'onglet *Connections*\n\n![](/images/rstudio_connections_dsn.png){width=\"515\"}\n\nSinon, vous aurez besoin d'entrer les paramètres suivants :\n\n-   *user* = \"...\" pour le nom d'utilisateur\n\n-   *password* = \"...\" pour le mot de passe\n\n-   *host* et *port* si besoin\n\n-   *dbname* pour le nom de la base de données\n\nSi vous utilisez uniquement la console, vous pouvez rentrer directement les paramètres dans `dbConnect()`, en arguments de la fonction.\\n\n\nUne fois connecté, on peut requêter ses données avec `{dplyr}` ou en SQL, en utilisant `dbGetQuery()`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DBI)\nlibrary(odbc)\nconn <- DBI::dbConnect(odbc::odbc() ,\"server-name\", database = \"MA_BASE\")\n\n# SQL\nma_table <- DBI::dbGetQuery(conn, \n                            \"SELECT * FROM MA_TABLE\n                            WHERE ...\")\n\n\n# DPLYR\nlibrary(dplyr)\nma_table_2 <- dplyr::tbl(conn, \"MA_TABLE\") |>  # ici, on créé une connexion à la table\n  dplyr::filter(...) |>  # on créé la requête\n  dplyr::collect() # et on collecte le résultat dans R avec collect()\n```\n:::\n\n\n\nPour interagir avec une base SQL pour autre chose qu'une requête (UPDATE, DROP, ...), on peut utiliser la librairie `{dbplyr}`, un back-end de `{dplyr}` pour les bases de données, ou utiliser `DBI::dbExecute()`\n\n# Cloud\n\nR dispose de nombreuses librairies pour récupérer des données hébergées sur un serveur cloud. \\n Je n'entrerai pas dans les détails de cette partie, car je n'ai pas encore eu beaucoup l'occasion de pratiquer par moi-même, mais je penserai à la mettre à jour dès que possible. \\n\n\n## Google Cloud Platform\n\nVoici une liste non exhaustive des librairies permettant de travailler avec Google Cloud Platform, disponible sur la vignette de [`{googleAuthR}`](https://code.markedmondson.me/googleAuthR/)\n\n-   [googleComputeEngineR](https://cloudyr.github.io/googleComputeEngineR/) - Google Compute Engine VMs API\n\n-   [searchConsoleR](https://code.markedmondson.me/searchConsoleR/) - Search Console API\n\n-   [bigQueryR](https://code.markedmondson.me/bigQueryR/) - BigQuery API. Part of the cloudyr project.\n\n-   [googleAnalyticsR](https://code.markedmondson.me/googleAnalyticsR/) - Google Analytics API\n\n-   [googleTagManagerR](https://github.com/IronistM/googleTagManageR) - Google Tag Manager API by IronistM\n\n-   [googleID](https://github.com/MarkEdmondson1234/googleID) - Simple user info from G+ API for Shiny app authentication flows.\n\n-   [googleCloudStorageR](https://code.markedmondson.me/googleCloudStorageR/) - Google Cloud Storage API\n\n-   [RoogleVision](https://github.com/cloudyr/googleCloudVisionR) - R Package for Image Recogntion, Object Detection, and OCR using the Google’s Cloud Vision API\n\n-   [googleLanguageR](https://github.com/ropensci/googleLanguageR) - Access Speech to Text, Entity analysis and translation APIs from R\n\n-   [googleCloudRunner](https://code.markedmondson.me/googleCloudRunner/) - Continuous Development and Integration with Cloud Run, Cloud Scheduler and Cloud Build\n\n## AWS\n\nLa documentation officielle de Amazon Web Service dispose [d'un tutoriel](https://aws.amazon.com/fr/blogs/opensource/getting-started-with-r-on-amazon-web-services/) pour accéder aux données du service depuis R.\n\n# API REST\n\nLe plus simple selon moi pour requêter une API REST avec R est d'utiliser l'excellent `{httr2}` de Hadley Wickham (que vous connaissez déjà certainement si vous utilisez le Tidyverse). La documentation du package est disponible [ici](https://httr2.r-lib.org/)\n\nLa première chose à faire est évidemment de lire la documentation de l'API que l'on souhaite requêter. Pour vous entraîner, vous pouvez utiliser [une des API disponibles sur data.gouv.fr](https://www.data.gouv.fr/dataservices). Pour cet exemple, j'utiliserai [l'API de Hub'eau pour la qualité de l'eau potable en France](https://hubeau.eaufrance.fr/page/api-qualite-eau-potable#console). \\n\n\nUne fois rendus sur le site, la documentation nous indique que la \"Base URL\" est hubeau.eaufrance.fr/api, puis nous indique les extensions à ajouter selon ce que l'on souhaite requêter (liste des communes ou résultats), puis les parmètres de filtrage\n\nOn utilisera également la librairie `{jsonlite}` pour convertir les données récupérées au format JSON en un data.frame\n\n::: callout-tip\n## Astuce\nLe package `{httr2}` nécessite l'installation du package `{curl}`, qui peut poser problème si vous êtes sur un réseau professionnel protégé par un pare-feu. Dans ce cas, contactez votre DSI, ou ... faites un partage de connexion depuis votre téléphone 🤫\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(httr2)\nlibrary(jsonlite)\n\nbase_url <- \"http://hubeau.eaufrance.fr/api\"\n\nreq <- request(base_url) |>  # pointer vers l'url de base\n  req_url_path_append(\"/v1/qualite_eau_potable/resultats_dis\") |> #extension\n  req_url_query(code_departement = \"85\") |> # paramètres de la requête\n  req_url_query(size = \"50\") |> \n  req_url_query(    \n    fields = c('libelle_parametre','libelle_parametre_maj',\n               'resultat_numerique', 'libelle_unite',\n               'limite_qualite_parametre','reference_qualite_parametre',\n               'nom_commune','date_prelevement',\n               'conclusion_conformite_prelevement'), \n    .multi = \"comma\")\n\nresp <- req_perform(req)\n\ndf_data <- resp |>\n  resp_body_string() |>\n  fromJSON()\ndf_data <- df_data$data\n\nhead(df_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                                    libelle_parametre\n1                  Cel. de cyanobactéries toxinogènes\n2 Cyanobactéries toxinogènes (exprimées en biovolume)\n3           Numération des cellules de phytoplanctons\n4                          Cellules de cyanobactéries\n5                              Microcystine-RR totale\n6          Somme des microcystines analysées (calcul)\n                       libelle_parametre_maj resultat_numerique  libelle_unite\n1         CEL. DE CYANOBACTÉRIES TOXINOGÈNES              0.041 n(cellules)/mL\n2                 CYANOBACTÉRIES TOXINOGÈNES              0.732          mm3/L\n3                 CELLULES DE PHYTOPLANCTONS              1.000 n(cellules)/mL\n4                 CELLULES DE CYANOBACTÉRIES              0.041 n(cellules)/mL\n5                     MICROCYSTINE-RR TOTALE              0.000           µg/L\n6 SOMME DES MICROCYSTINES ANALYSÉES (CALCUL)              0.000           µg/L\n  limite_qualite_parametre reference_qualite_parametre nom_commune\n1                     <NA>                        <NA>    APREMONT\n2                     <NA>                        <NA>    APREMONT\n3                     <NA>                        <NA>    APREMONT\n4                     <NA>                        <NA>    APREMONT\n5                 <=1 µg/L                        <NA>    APREMONT\n6                 <=1 µg/L                        <NA>    APREMONT\n      date_prelevement\n1 2025-06-30T14:00:00Z\n2 2025-06-30T14:00:00Z\n3 2025-06-30T14:00:00Z\n4 2025-06-30T14:00:00Z\n5 2025-06-30T13:55:00Z\n6 2025-06-30T13:55:00Z\n                                                                        conclusion_conformite_prelevement\n1 Eau d'alimentation conforme aux exigences de qualité en vigueur pour l'ensemble des paramètres mesurés.\n2 Eau d'alimentation conforme aux exigences de qualité en vigueur pour l'ensemble des paramètres mesurés.\n3 Eau d'alimentation conforme aux exigences de qualité en vigueur pour l'ensemble des paramètres mesurés.\n4 Eau d'alimentation conforme aux exigences de qualité en vigueur pour l'ensemble des paramètres mesurés.\n5 Eau d'alimentation conforme aux exigences de qualité en vigueur pour l'ensemble des paramètres mesurés.\n6 Eau d'alimentation conforme aux exigences de qualité en vigueur pour l'ensemble des paramètres mesurés.\n```\n\n\n:::\n:::\n\n\n\n::: callout-tip\n## Cacher les clés API\nDans le cas où vous utilisez une API privée nécessitant une clé, il est judicieux d'éviter de stocker vos identifiants dans le script R. Pour cela, vous pouvez utiliser un fichier [.Rprofile](https://docs.posit.co/ide/user/ide/guide/environments/r/managing-r.html), stocké sur votre machine locale, et qui se lance quand vous démarrez votre IDE.\nAinsi, vos identifiants sont présents dans votre environnement sans avoir à les déclarer dans le script. Si vous hébergez votre code sur GitHub, vous pouvez inclure le .Rprofile dans le .gitignore pour éviter de les inclure dans le repo, et utiliser la fonction [*Github Secrets*](https://docs.github.com/fr/actions/how-tos/write-workflows/choose-what-workflows-do/use-secrets) pour remplacer le .Rprofile.\n:::\n\n\n\n# Webscraping  \n\nSi vous souhaitez récupérer des données sur un site web qui ne propose pas d'API, l'une des solutions est le [*web scraping*](https://fr.wikipedia.org/wiki/Web_scraping). \n\n\n::: callout-warning\n## Attention\nTous les sites n'autorisent pas le web scraping, et les données doivent être *nativement* publiques, *tombées* dans le domaine publique ou sous licence libre *si votre usage n'est pas commercial*. Renseignez-vous avant de scraper une page !\n:::\n\nLe principal package de web scraping en R est [`{rvest}`](https://rvest.tidyverse.org/index.html) (toujours développé par Hadley Wickham). \nPour cet exemple, j'utiliserai aussi [`{polite}`](https://dmi3kno.github.io/polite/index.html), un package qui permet de suivre 3 règles d'éthique lors d'une session de scraping : *\"Demander la permission, prendre doucement et ne jamais demander deux fois\"*.\nCes règles permettent d'éviter de causer des problèmes en récoltant des données non autorisées ou en surchargeant le serveur de requêtes, et nous éviterons d'être bannis par les administrateurs du site scrapé.\n\nDans cet exemple, nous allons récupérer une table contenant la liste des communes de Vendée, sur [cette page wikipédia](https://fr.wikipedia.org/wiki/Liste_des_communes_de_la_Vend%C3%A9e)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rvest)\nlibrary(polite)\n\nurl <- \"https://fr.wikipedia.org/wiki/Liste_des_communes_de_la_Vend%C3%A9e\"\nsession <- polite::bow(url) \n# Bow permet d'interroger le robots.txt et nous informe du résultat\n# il enregistre notamment le délai minimum de requêtage autorisé par le site\n\nsession\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<polite session> https://fr.wikipedia.org/wiki/Liste_des_communes_de_la_Vend%C3%A9e\n    User-agent: polite R package\n    robots.txt: 261 rules are defined for 34 bots\n   Crawl delay: 5 sec\n  The path is scrapable for this user-agent\n```\n\n\n:::\n\n```{.r .cell-code}\npage <- polite::scrape(session)\n# scrape() récupère le contenu autorisé\n\ncities_table <- page |> \n  rvest::html_element(\"table.wikitable\") |> \n  rvest::html_table()\n\n# html_element récupère le premier élément de la classe \"table.wikitable\"\n# pour tous les récupérer sous forme de liste : html_elements()\n# html_table() permet de convertir des données tabulaires en un dataframe\n\nhead(cities_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 10\n  Nom             CodeInsee `Code postal` Arrondissement Canton Intercommunalité\n  <chr>               <int>         <dbl> <chr>          <chr>  <chr>           \n1 La Roche-sur-Y…     85191         85000 La Roche-sur-… La Ro… CA La Roche-sur…\n2 Les Achards         85152         85150 Les Sables-d’… Talmo… Pays-des-Achards\n3 L'Aiguillon-la…     85001         85460 Les Sables-d'… Mareu… CC Sud-Vendée-L…\n4 L'Aiguillon-su…     85002         85220 Les Sables-d'… Saint… CA Pays de Sain…\n5 Aizenay             85003         85190 La Roche-sur-… Aizen… CC de Vie-et-Bo…\n6 Angles              85004         85750 Les Sables-d'… Mareu… CC Vendée-Grand…\n# ℹ 4 more variables: `Superficie(km2)` <chr>,\n#   `Population(dernière pop. légale)` <chr>, `Densité(hab./km2)` <chr>,\n#   Modifier <lgl>\n```\n\n\n:::\n:::\n\n\n\n\n\n# OCR et PDF  \n\nDans le domaine de la santé, les compte-rendus médicaux contiennent énormément de données intéressantes, mais sont malheureusement difficilement exploitables car dans un format non-structuré : un papier scanné et stocké en PDF.\n\nL'OCR (Optical Character Recognition) est un système de *machine learning* permettant d'extraire le texte présent sur une image, parfait donc pour exploiter un grand nombre de comtpe-rendus scannés sans avoir à le faire manuellement.\n\nDans cet exemple, nous utiliserons le modèle **Tesseract**, développé par Google et aujourd'hui Open Source. Nous utiliserons également la librairie `doParallel` pour paralléliser le traitement des images et augmenter la vitesse d'OCRisation\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tesseract)\nlibrary(doParallel)\nlibrary(pdftools)\n\n# Récupérer le chemin de tous les fichiers PDF présents dans un dossier\nlist_pdf <- list.files(path = \"path/to/folder\", \n                       pattern = \"\\\\.pdf$\", \n                       all.files = TRUE, \n                       full.names = TRUE,\n                       recursive = FALSE\n                       )\n\nncores <- doParallel::detectCores(logical = FALSE) # compter les coeurs physiques \n\n# Créer le cluster, en laissant 2 coeurs non utilisés\ncl <- doParallel::makeCluster(ncores-2)   \n# et pour arrêter : stopCluster(cl)\n\n\n# Charger les librariries et les objets dans les clusters\n## clusterEvalQ() : exécute le code dans le cluster\nclusterEvalQ(cl, {\n  library(tidyverse);\n  library(tesseract);\n  library(pdftools)\n  })\n\n## clusterExport() charge l'objet dans le cluster\nclusterExport(cl, c(\"list_pdf\"))\n\n\n# Créer une fonction qui renvoie un dataframe en sortie (1 ligne par page)\n\nmyPdfConvert <- function(list_objet) { \n  list_objet %>% map_df(~ data.frame(doc_origin = .x, #.x = chaque élément de listobjet\n                                     texte = ocr(pdftools::pdf_convert(.x, dpi = 200)) # océrise un pdf converti en png\n                                     ))\n}\n\n\n# Appliquer la fonction myPdfConvert à chaque élément de list_pdf\nres <- parLapply(cl, list_pdf, myPdfConvert)\n# Le résultat est obtenu sous forme de liste (1 élément par cluster) => les recombiner\nres <- do.call(\"rbind\", res)\n\n# Arrêter le cluster\nstopCluster(cl)\n\n\n# Pour récupérer numéro de page et nom du doc à chaque ligne (1 ligne = 1 page)\nresr <- res %>% \n  group_by(doc_origin) %>% \n  mutate(doc_page = 1:n(), doc_nom = str_extract_all(doc_origin, \"[[:alnum:]_ ]*\\\\.pdf\")) %>%\n  ungroup()\n```\n:::\n\n\n\n\nPour visualiser l'impact de la parallélisation : \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](decouvrir_connections_files/figure-html/ocr2-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n# JSON et XML  \n\nLes formats **XML** (eXtensible Markup Language) et **JSON** (JavaScript Object Notation) sont largement utilisés pour le stockage et l’échange de données, notamment dans les **API REST**, les **flux RSS**, ou le stockage de données hiérarchiques .\n\nPour lire les fichiers XML, on utilisera`{xml2}` et pour JSON :`{jsonlite}` (il en existe de nombreuses autres, mais ce sont les plus connues)\n\n## XML  \n\nSupposons que nous ayons un fichier \"livres.xml\" ressemblant à cela :\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(xml2)\n\nxml_doc <- as_xml_document(\n\"<livres>\n  <livre>\n    <titre>R pour les débutants</titre>\n    <auteur>Jean Martin</auteur>\n    <annee>2020</annee>\n  </livre>\n  <livre>\n    <titre>Analyse de données avec R</titre>\n    <auteur>Marie Dupont</auteur>\n    <annee>2021</annee>\n  </livre>\n</livres>\"\n)\n```\n:::\n\n\n\nPour lire le fichier .xml, on utilisera\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxml_doc <- xml2::read_xml(\"livres.xml\")\n```\n:::\n\n\n\nEnsuite, on peut extraire les titres avec\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitres_xml <- xml_find_all(xml_doc, \".//livre/titre\")\nprint(titres_xml)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{xml_nodeset (2)}\n[1] <titre>R pour les débutants</titre>\n[2] <titre>Analyse de données avec R</titre>\n```\n\n\n:::\n:::\n\n\n\nLes fonctions `xml_find_*()` permettent d'utiliser des expression **xpath**, similaire à du **REGEX** mais pour les architectures en arbre et renvoient des objets de classe `xml_nodeset`. Pour récupérer un vecteur `character`, on rajoute simplement `xml_text()`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitres <- xml_text(titres_xml)\nprint(titres)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"R pour les débutants\"      \"Analyse de données avec R\"\n```\n\n\n:::\n:::\n\n\n\n\n## JSON  \n\nPour lire du JSON, on utilisera simplement `fromJSON()`, pour convertir une chaîne de texte, ou `read_json()` pour lire un fichier .json. Les deux fonctions renvoient directement un `data.frame`\n\n\n\n\n```{.r .cell-code}\nlibrary(jsonlite)\n\nlivres_json <- fromJSON(\n'\n[\n  {\n    \"titre\": \"R pour les débutants\",\n    \"auteur\": \"Jean Dupont\",\n    \"annee\": 2020\n  },\n  {\n    \"titre\": \"Analyse de données avec R\",\n    \"auteur\": \"Marie Curie\",\n    \"annee\": 2021\n  }\n]\n')\n\nprint(livres_json)\n```\n\n                      titre      auteur annee\n1      R pour les débutants Jean Dupont  2020\n2 Analyse de données avec R Marie Curie  2021\n\n\n\nOn citera aussi la possibilité de convertir un objet R en JSON avec la fonction `toJSON()`. \n\n\n\n# DICOM  \n\nSans rentrer dans les détails ici, le format DICOM (Digital Imaging and Communications in Medicine) est un standard pour l'imagerie médicale. le package `{Espadon}` [développé par le CNRS](https://espadon.cnrs.fr/) permet d'interagir avec ce format\n\n# GPX  \n\nGPX est un format pour les données géospatiales, qui permet par exemple de créer un itinéraire sur une carte. [Cet article](https://www.appsilon.com/post/r-gpx-files) explique très bien comment lire ce standard, souvent stocké au format XML.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "decouvrir_connections_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}