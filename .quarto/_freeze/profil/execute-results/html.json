{
  "hash": "341af9a4707e2b5a62c94d3da2ca5f4c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Profil\"\npage-layout: full\ntoc: true\n---\n\n\n\n\n# A propos de moi\n\nIssu d'un parcours en Biologie et Recherche Clinique, je me suis rapidement dirigé vers le traitement de données de santé à la fin de l'année de Master 1, en me formant en autodidacte au langage R.\n\nMon parcours professionnel m'a amené à travailler dans le milieu du Dispositif Médical (DM) en orthopédie, puis dans les études de vie réelle (RWD) en cancérologie.\n\nMes missions actuelles portent principalement sur le nettoyage, la qualification de données de vie réelle et le requêtage d'Entrepôts de Données de Santé, mais également de la Data Visualisation, du Data Engineering et le développement d'applications Shiny ou de dashboards.\n\n# Stack technique\n\nEn dehors de R (détaillé après) :\n\n-   Programmation :\n\n    -   Maîtrise : SQL\n\n    -   Notions : HTML, CSS, Python, Shell\n\n-   Visualisation :\n\n    -   Power BI, Qlik Sense\n\n-   CI/CD :\n\n    -   Git (GitHub, GitLab), Docker\n\n-   ETL :\n\n    -   Talend\n\n-   e-CRF/EDC :\n\n    -   REDCap, EasyMedStats\n\n# Missions\n\n-   Participation à la création et la gestion d'Entrepôts de Données de Santé\n\n-   Manipulation de données de vie réelle dans le cadre d'études RNIPH, incluant :\n\n    -   Qualification (vérification des formats, tests de cohérence, de chronologie ...)\n\n    -   Nettoyage de données, reformatage\n\n    -   Fouille de texte (requêtes REGEX)\n\n-   Développement d'applications Shiny et documents Quarto (Rapports, Dashboards, Catalogues de données)\n\n-   Automatisation de process / ETL (Exécution de scripts via un orchestrateur type Airflow, jobs Talend)\n\n-   Requêtage d'APIs\n\n-   Création de formulaires de saisie (e-CRF)\n\n-   Enseignement : Introduction à R et à la gestion de données, auprès d'étudiants de Licence 2 et Master 2 de biologie\n\n### Données temporelles\n\n### Données géographiques\n\n## Analyse & Visualisation\n\nggplot2 et dérivés,\n\ngwalkr\n\nstats\n\n## Reporting\n\n### Rmarkdown\n\n### Quarto\n\n### R Shiny\n\n## Data Engineering\n\n### Orchestrateur\n\n{maestro} est un package R permettant d'orchestrer l'exécution programmée de différents scripts R, permettant ainsi de créer de gérer des pipelines de données.\n\n### Bases de données (SQL)\n\n### Big Data\n\n#### Apache Arrow et DuckDB\n\nGrâce à Apache Arrow et Duckdb, il est possible de traiter de grands volumes de données facilement avec R. Ci-dessous, un exemple de requête sur un jeu de données de plus d'1 milliard de lignes. \\n\n\nLes données sont celles du dataset TLC Trip Record Data (https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page), et contiennent les informations de trajets de taxis new-yorkais. \\n\n\nDans cet exemple, les données de chaque mois entre janvier 2013 et avril 2025 ont été récupérées et regroupées dans un dossier appelé taxi_dataset. Ce dossier contient 147 fichiers au format .Parquet et pèse 14Go (le format .Parquet est un format compressé, le même jeu de données en .csv pèse environ 100Go). \\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) # Tidyverse sert à la manipulation de données\nlibrary(duckdb) # duckdb pour créer une base duckdb virtuelle\nlibrary(arrow) # Arrow pour lire des fichiers .Parquet\nlibrary(duckplyr) # Duckplyr pour utiliser dplyr avec le moteur duckdb\nlibrary(DBI) # DBI pour créer une connection à la base duckdb virtuelle\nlibrary(microbenchmark) # Pour vérifier les temps d'exécution\n\n# Créer une base duckdb virtuelle\nconn_ddb <- dbConnect(duckdb(), dbdir = \":memory:\")\n\nyellow_taxi <- conn_ddb %>% \n  tbl(\"read_parquet('D:\\\\taxi_dataset\\\\*.parquet')\" )\n# Il suffit d'indiquer le chemin vers le dossier contenant les 157 fichiers pour que {arrow} reconstitue le tout et le considère comme un seul dataset (il faut évidemment que les jeux de données suivent le même schéma)\n# Ici, yellow_taxi ne contient pas directement les données, mais une connexion au dataset.\n# on peut désormais requêter yellow_taxi comme un dataframe classique avec la grammaire de {dplyr}, sans avoir chargé les données dans la mémoire vive de R\n\nyellow %>% count()\n# 1.101.292.583 => 1 milliard de lignes\n\n# Utiliser microbenchmark() pour calculer le temps d'éxécution\nmicrobenchmark(\n    res = x <- yellow %>% \n        filter(trip_distance >=2) %>% # filtrer les lignes\n        summarize(mean_tip_amount = mean(tip_amount), .by = passenger_count) %>% \n      # Aggréger tip_amount pour obtenir la moyenne du pourboire par nombre de passagers\n        collect(), # Ajouter collect() pour récupérer les données\n    times = 1\n)\n# 11.5 secondes\n\nmicrobenchmark(\n  res = x <- yellow %>% \n    filter(trip_distance >=2) %>%  \n    mutate(year = year(tpep_pickup_datetime)) %>% # Créer une colonne \"year\" qui contient l'année du voyage\n    summarize(mean_tip_amount = mean(tip_amount), .by = year) %>% # aggréger tip_amount pour obtenir la moyenne de pourboire par année\n    collect(),\n  times = 1\n)\n# 14.2 secondes !\n```\n:::\n\n\n\n\n#### Spark et Hadoop\n\nOn peut aussi se connecter à un cluster Spark, et envoyer ses requêtes depuis R avec la syntaxe de {dplyr} en utilisant {sparklyr}.\n\n### API\n\nhittr2\n\n### CI/CD\n\nGitHub, testthat, usethis\n\nDocker (dockerfiler, rocker, shiny2docker, dockitect ...\n\n### Cloud\n\nun exemple d'interaction avec différents services d'un cloud (stockage, faire tourner R sur un serveur...)\n\n# Roadmap\n\nLes compétences que je souhaite développer dans les mois à venir, au 15 juin 2025 :\n\n-   Finaliser la maîtrise de Duckdb et Arrow pour le Big Data (Juillet 2025) ✅\n\n-   Développer mes compétences CI/CD (Automne 2025)\n\n    -   Être à l'aise avec le déploiement d'applis Shiny et de documents Quarto sur différents outils de publication\n\n    -   Développer des applis Shiny *Production-grade* avec {rhino} et {golem}, en intégrant des tests unitaires, d'intégration et End-to-End\n\n    -   Apprendre la dockerisation et déployer au moins une appli Shiny dans un docker\n\n-   Découvrir un client Cloud (GCP/AWS/Azure) (Fin 2025)\n\n-   M'initier au Machine Learning (mi-2026)\n\n    -   Remise à niveau en statistiques\n\n    -   Me former à l'utilisation du package {tidymodels}\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}