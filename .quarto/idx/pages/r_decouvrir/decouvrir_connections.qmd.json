{"title":"Recueil de donn√©es","markdown":{"yaml":{"title":"Recueil de donn√©es","description":"Se connecter √† tout type de donn√©es","format":"html","author":"F√©lix Marchais","date":"2025-07-29","categories":["R","Bases de donn√©es"],"execute":{"eval":false,"echo":true,"message":false,"warning":false}},"headingText":"CSV en local","containsRefs":false,"markdown":"\n\n\nR est un langage qui permet d'int√©ragir avec √† peu pr√®s n'importe quel format de donn√©es : \\n\n\n-   Des fichiers tabulaires (CSV, Excel, Parquet...), en local ou sur un serveur\n\n-   Des bases SQL/NoSQL : MS SQL Server, MySQL, MongoDB, Oracle, DuckDB...\n\n-   Des donn√©es h√©berg√©es sur un cloud (GCP, AWS, Azure...)\n\n-   Des donn√©es r√©cup√©r√©es via du webscraping ou une requ√™te API\n\n-   Des donn√©es au format JSON ou XML\n\nMais pas seulement ! R peut √©galement travailler avec des images (OCRiser un PDF pour r√©cup√©rer son contenu, ou en g√©n√©rer un), des fichiers DICOM, et bien plus encore.\n\n\nCommen√ßons avec le plus simple : les fichiers CSV\n\n```{r csv}\nlibrary(readr) # package du Tidyverse\nread_csv(\"path/to/iris.csv\")\n```\n\n\n```{r}\n#| eval: true\n#| echo: false\nhead(iris)\n```\n\n\n\nOn fait difficilement plus facile. On notera tout de m√™me que `{readr}` offre des arguments et fonctions suppl√©mentaires pour g√©rer diff√©rents probl√®mes que l'on rencontre souvent avec le CSV : les s√©parateurs et l'encodage. \\n\n\nLa fonction `read_csv` lit par d√©faut des fichiers au \"format international\" dont le s√©parateur est une virgule (,) et la d√©cimale un point (.). Cependant, en France (et en Europe), on utilis√© g√©n√©ralement la virgule comme s√©parateur d√©cimal. On a donc invent√© le format csv avec s√©parateur point-virgule (;) et d√©cimale en virgule (,). Pour lire un fichier sous ce format, on peut utiliser la fonction `read_csv2()` de `{readr}`. \\n\n\nPour des formats encore plus exotiques (tsv par exemple), `read_delim()` permet de pr√©ciser le d√©limiteur. \\n\n\nOn notera aussi que les fonctions `read_*()` fournissent des arguments permettant d'expliciter les valeurs nulles, de retirer les espaces en d√©but/fin de cha√Æne (ce qui arrive tr√®s souvent sur des donn√©es saisies √† la main dans Excel), de diff√©rencier des noms de colonnes en doublons, ou encore de sauter les premi√®res lignes d'un fichier.\n\n```{r csv_2}\nlibrary(readr)\nread_csv2(\"path/to/file_fr.csv\", # lire un fichier au format FR, delim = ;\n          na = c(\"\",\"NULL\",\"NA\"), # les cases contenant \"\",\"NULL\" et \"NA\" seront vides\n          trim_ws = TRUE, # trim_whitespace : retirer les espaces en fin de cha√Æne\n          skip = 1, # sauter la premi√®re ligne\n          name_repair = \"unique\" # diff√©rencier les noms en doublons\n)\n```\n\n# CSV √† distance\n\nLes fonctions `read.*()` (en base R) et `read_*()` (`{readr}`) permettent toutes de charger des fichiers CSV via une URL.\n\n```{r read_csv_url}\n\ncovid_data <- readr::read_csv2(\"https://www.data.gouv.fr/api/1/datasets/r/fe3e7099-a975-4181-9fb5-2dd1b8f1b552\")\nhead(covid_data)\n```\n\n```{r read_csv_url_2}\n#| eval: true\n#| echo: false\n\nhead(readr::read_csv(\"../../data/covid_data.csv\"))\n```\n\n\n\n# Excel\n\nLes fichiers dits \"Excel\" ont l'extension .xls ou .xlsx, et peuvent contenir diff√©rents feuillets, de la mise en forme, des cellules fusionn√©es, des formules, des commentaires et des graphes. \\n\n\nPour lire ces donn√©es, il existe plusieurs librairies : `{readxl}`, `{openxslx}` ou `{openxslx2}`\n\n```{r readxl}\n\nlibrary(readxl)\nread_excel(\"path/to/file.xslx\",\n           sheet = 1, # feuillet, par num√©ro ou par nom\n           range = \"B3:D87\", # les cellules √† garder (optionnel)\n           na = c(\"\",\"NULL\",\"NA\"), # les cellules √† consid√©rer vides\n           .name_repair = \"unique\" # diff√©rencier les noms en doublons\n)\n\n```\n\n# Parquet\n\nLe format .parquet, encore malheureusement peu connu, est un format compress√© et orient√© en colonnes, orient√© vers la performance. Un m√™me jeu de donn√©es au format parquet est entre 5 et 10 fois moins volumineux qu'en csv et est optimis√© pour √™tre lu rapidement par R ou Python. On citera aussi sa compatibilit√© avec DuckDB pour former un duo parfait pour [travailler le Big Data avec R](decouvrir_bigdata.qmd). L'un des rares d√©fauts qu'on peut lui trouver est de ne pas √™tre compatible avec Excel, ce qui limite sa diffusion. \\n\n\nSi vous ne l'avez pas encore lu, je vous renvoie vers cet excellent article : [Parquet devrait remplacer le format CSV](https://www.icem7.fr/cartographie/parquet-devrait-remplacer-le-format-csv/)\n\nIl existe deux m√©thodes principales pour lire un fichier parquet en R : `{arrow}`, la librairie de [Apache Arrow](https://arrow.apache.org/) et `{duckDB}`, la librairie de [DuckDB](https://duckdb.org/).\n\nLes deux m√©thodes permettent de cr√©er une *connexion* vers le jeu de donn√©es, au lieu de les importer dans la m√©moire de R, permettant de travailler avec des donn√©es pus volumineuses que la RAM. Les deux m√©thodes permettent d'utiliser `{dplyr}` pour le requ√™tage, mais `{duckdb}` permet aussi de requ√™ter n'importe quel fichier plat en SQL. \\n\n\nOn notera √©galement la capacit√© √† lire plusieurs fichiers regroup√©s dans un dossier (ayant le m√™me sch√©ma), voire m√™me des fichiers partitionn√©s ([au style Hive](https://arrow.apache.org/docs/r/reference/hive_partition.html)), permettant de ne scanner que les donn√©es n√©cessaires, pour optimiser encore plus la performance de requ√™tage.\n\n```{r parquet}\n\n# Arrow : \nlibrary(arrow)\narrow_df <- read_parquet(\"path/to/file.parquet\")\n\n# Il est possible de lire directement un ensemble de fichiers .parquet ayant le m√™me sch√©ma \n# et regroup√©s dans un dossier (donn√©es par batch, avec un fichier par mois par exemple)\narrow_dataset <- open_dataset(\"path/to/folder\")\n# Ici, arrow_dataset est une connexion, de classe `Dataset`\n# Un `Dataset` de {arrow} se requ√™te avec {dplyr}\n# pour r√©cup√©rer les donn√©es dans l'environnement de R : \narrow_df <- arrow_dataset |> \n  filter(...) |>  # la plupart des fonctions de {dplyr} sont compatibles\n  select(...) |>  \n  collect() # pour ex√©cuter la requ√™te et r√©cup√©rer un tibble/dataframe\n\n\n\n# DuckDB :\nlibrary(duckdb)\nlibrary(DBI) # DataBase Interface\nlibrary(dplyr)\n\nconn_ddb <- DBI::dbConnect(duckdb(), dbdir = \":memory:\") # Cr√©er une base duckdb virtuelle\nduck_df <- conn_ddb |> \n  tbl(\"path/dataset/**/*.parquet\") |> \n  filter(...)\n# Le ** signifie \"tout\", et permet de lire tous les fichiers .parquet du dossier\n# Dans un fichier partitionn√© par exemple\n\n```\n\n\n:::{.callout-tip}\n## Note  \n{arrow} renvoie des objets de type `arrow_table` ou `Dataset`, qui ne sont compatibles qu'avec les fonctions de `{dplyr}`. Si vous souhaitez modifier une colonne avec une fonction de `{purrr}`, `{lubridate}` ou `{stringr}`, il faudra d'abord utiliser `collect()` pour obtenir un `tibble`. \\n\n\nA l'inverse, `{duckdb}` renvoie des objets de classe `tbl` (donc des `tibble`). Si l'on ajoute l'incroyable performance de requ√™tage du moteur duckdb, j'aurais plut√¥t tendance √† privil√©gier cette m√©thode\n:::\n\n:::{.callout-important}\n## Info  \nR√©cemment, `{duckplyr}`, une librairie int√©grant `{dplyr}` avec le moteur de `{duckdb}` [a rejoint le `Tidyverse`](https://www.tidyverse.org/blog/2025/06/duckplyr-1-1-0/), signe que Duckdb est per√ßu comme un outil d'avenir.\n\n:::\n\n\n# Connexions aux bases SQL\n\nSi vous utilisez Rstudio, le moyen le plus simple de se connecter √† une source de donn√©es est d'utiliser l'onglet *Connections*, g√©n√©ralement situ√© en haut √† droite, avec votre *Environnement*. En cliquant sur *New connection*, une fen√™tre appara√Æt et va automatiquement vous proposer les sources √† disposition. \\n\n\n![Exemple des connexions existantes sur mon poste](/images/rstudio_connections.png)\n\nSi vous avez d√©j√† configur√© un DSN contenant vos identifiants, les informations de connexion devraient d√©j√† √™tre remplies, vous n'avez plus qu'√† cliquer sur \"OK\", et le code s'ex√©cutera dans la console. Vous pourrez alors visualiser vos bases dans l'onglet *Connections*\n\n![](/images/rstudio_connections_dsn.png){width=\"515\"}\n\nSinon, vous aurez besoin d'entrer les param√®tres suivants :\n\n-   *user* = \"...\" pour le nom d'utilisateur\n\n-   *password* = \"...\" pour le mot de passe\n\n-   *host* et *port* si besoin\n\n-   *dbname* pour le nom de la base de donn√©es\n\nSi vous utilisez uniquement la console, vous pouvez rentrer directement les param√®tres dans `dbConnect()`, en arguments de la fonction.\\n\n\nUne fois connect√©, on peut requ√™ter ses donn√©es avec `{dplyr}` ou en SQL, en utilisant `dbGetQuery()`\n\n```{r db_get_query}\nlibrary(DBI)\nlibrary(odbc)\nconn <- DBI::dbConnect(odbc::odbc() ,\"server-name\", database = \"MA_BASE\")\n\n# SQL\nma_table <- DBI::dbGetQuery(conn, \n                            \"SELECT * FROM MA_TABLE\n                            WHERE ...\")\n\n\n# DPLYR\nlibrary(dplyr)\nma_table_2 <- dplyr::tbl(conn, \"MA_TABLE\") |>  # ici, on cr√©√© une connexion √† la table\n  dplyr::filter(...) |>  # on cr√©√© la requ√™te\n  dplyr::collect() # et on collecte le r√©sultat dans R avec collect()\n```\n\nPour interagir avec une base SQL pour autre chose qu'une requ√™te (UPDATE, DROP, ...), on peut utiliser la librairie `{dbplyr}`, un back-end de `{dplyr}` pour les bases de donn√©es, ou utiliser `DBI::dbExecute()`\n\n# Cloud\n\nR dispose de nombreuses librairies pour r√©cup√©rer des donn√©es h√©berg√©es sur un serveur cloud. \\n Je n'entrerai pas dans les d√©tails de cette partie, car je n'ai pas encore eu beaucoup l'occasion de pratiquer par moi-m√™me, mais je penserai √† la mettre √† jour d√®s que possible. \\n\n\n## Google Cloud Platform\n\nVoici une liste non exhaustive des librairies permettant de travailler avec Google Cloud Platform, disponible sur la vignette de [`{googleAuthR}`](https://code.markedmondson.me/googleAuthR/)\n\n-   [googleComputeEngineR](https://cloudyr.github.io/googleComputeEngineR/) - Google Compute Engine VMs API\n\n-   [searchConsoleR](https://code.markedmondson.me/searchConsoleR/) - Search Console API\n\n-   [bigQueryR](https://code.markedmondson.me/bigQueryR/) - BigQuery API. Part of the cloudyr project.\n\n-   [googleAnalyticsR](https://code.markedmondson.me/googleAnalyticsR/) - Google Analytics API\n\n-   [googleTagManagerR](https://github.com/IronistM/googleTagManageR) - Google Tag Manager API by IronistM\n\n-   [googleID](https://github.com/MarkEdmondson1234/googleID) - Simple user info from G+ API for Shiny app authentication flows.\n\n-   [googleCloudStorageR](https://code.markedmondson.me/googleCloudStorageR/) - Google Cloud Storage API\n\n-   [RoogleVision](https://github.com/cloudyr/googleCloudVisionR) - R Package for Image Recogntion, Object Detection, and OCR using the Google‚Äôs Cloud Vision API\n\n-   [googleLanguageR](https://github.com/ropensci/googleLanguageR) - Access Speech to Text, Entity analysis and translation APIs from R\n\n-   [googleCloudRunner](https://code.markedmondson.me/googleCloudRunner/) - Continuous Development and Integration with Cloud Run, Cloud Scheduler and Cloud Build\n\n## AWS\n\nLa documentation officielle de Amazon Web Service dispose [d'un tutoriel](https://aws.amazon.com/fr/blogs/opensource/getting-started-with-r-on-amazon-web-services/) pour acc√©der aux donn√©es du service depuis R.\n\n# API REST\n\nLe plus simple selon moi pour requ√™ter une API REST avec R est d'utiliser l'excellent `{httr2}` de Hadley Wickham (que vous connaissez d√©j√† certainement si vous utilisez le Tidyverse). La documentation du package est disponible [ici](https://httr2.r-lib.org/)\n\nLa premi√®re chose √† faire est √©videmment de lire la documentation de l'API que l'on souhaite requ√™ter. Pour vous entra√Æner, vous pouvez utiliser [une des API disponibles sur data.gouv.fr](https://www.data.gouv.fr/dataservices). Pour cet exemple, j'utiliserai [l'API de Hub'eau pour la qualit√© de l'eau potable en France](https://hubeau.eaufrance.fr/page/api-qualite-eau-potable#console). \\n\n\nUne fois rendus sur le site, la documentation nous indique que la \"Base URL\" est hubeau.eaufrance.fr/api, puis nous indique les extensions √† ajouter selon ce que l'on souhaite requ√™ter (liste des communes ou r√©sultats), puis les parm√®tres de filtrage\n\nOn utilisera √©galement la librairie `{jsonlite}` pour convertir les donn√©es r√©cup√©r√©es au format JSON en un data.frame\n\n::: callout-tip\n## Astuce\nLe package `{httr2}` n√©cessite l'installation du package `{curl}`, qui peut poser probl√®me si vous √™tes sur un r√©seau professionnel prot√©g√© par un pare-feu. Dans ce cas, contactez votre DSI, ou ... faites un partage de connexion depuis votre t√©l√©phone ü§´\n:::\n\n```{r hubeau}\nlibrary(httr2)\nlibrary(jsonlite)\n\nbase_url <- \"http://hubeau.eaufrance.fr/api\"\n\nreq <- request(base_url) |>  # pointer vers l'url de base\n  req_url_path_append(\"/v1/qualite_eau_potable/resultats_dis\") |> #extension\n  req_url_query(code_departement = \"85\") |> # param√®tres de la requ√™te\n  req_url_query(size = \"50\") |> \n  req_url_query(    \n    fields = c('libelle_parametre','libelle_parametre_maj',\n               'resultat_numerique', 'libelle_unite',\n               'limite_qualite_parametre','reference_qualite_parametre',\n               'nom_commune','date_prelevement',\n               'conclusion_conformite_prelevement'), \n    .multi = \"comma\")\n\nresp <- req_perform(req)\n\ndf_data <- resp |>\n  resp_body_string() |>\n  fromJSON()\n\ndf_data <- df_data$data\n\nhead(df_data)\n\n```\n\n```{r hubeau_2}\n#| eval: true\n#| echo: false\n\nhead(readr::read_csv(\"../../data/hubeau_data.csv\"))\n```\n\n\n::: callout-tip\n## Cacher les cl√©s API\nDans le cas o√π vous utilisez une API priv√©e n√©cessitant une cl√©, il est judicieux d'√©viter de stocker vos identifiants dans le script R. Pour cela, vous pouvez utiliser un fichier [.Rprofile](https://docs.posit.co/ide/user/ide/guide/environments/r/managing-r.html), stock√© sur votre machine locale, et qui se lance quand vous d√©marrez votre IDE.\nAinsi, vos identifiants sont pr√©sents dans votre environnement sans avoir √† les d√©clarer dans le script. Si vous h√©bergez votre code sur GitHub, vous pouvez inclure le .Rprofile dans le .gitignore pour √©viter de les inclure dans le repo, et utiliser la fonction [*Github Secrets*](https://docs.github.com/fr/actions/how-tos/write-workflows/choose-what-workflows-do/use-secrets) pour remplacer le .Rprofile.\n:::\n\n\n\n# Webscraping  \n\nSi vous souhaitez r√©cup√©rer des donn√©es sur un site web qui ne propose pas d'API, l'une des solutions est le [*web scraping*](https://fr.wikipedia.org/wiki/Web_scraping). \n\n\n::: callout-warning\n## Attention\nTous les sites n'autorisent pas le web scraping, et les donn√©es doivent √™tre *nativement* publiques, *tomb√©es* dans le domaine publique ou sous licence libre *si votre usage n'est pas commercial*. Renseignez-vous avant de scraper une page !\n:::\n\nLe principal package de web scraping en R est [`{rvest}`](https://rvest.tidyverse.org/index.html) (toujours d√©velopp√© par Hadley Wickham). \nPour cet exemple, j'utiliserai aussi [`{polite}`](https://dmi3kno.github.io/polite/index.html), un package qui permet de suivre 3 r√®gles d'√©thique lors d'une session de scraping : *\"Demander la permission, prendre doucement et ne jamais demander deux fois\"*.\nCes r√®gles permettent d'√©viter de causer des probl√®mes en r√©coltant des donn√©es non autoris√©es ou en surchargeant le serveur de requ√™tes, et nous √©viterons d'√™tre bannis par les administrateurs du site scrap√©.\n\nDans cet exemple, nous allons r√©cup√©rer une table contenant la liste des communes de Vend√©e, sur [cette page wikip√©dia](https://fr.wikipedia.org/wiki/Liste_des_communes_de_la_Vend%C3%A9e)\n\n```{r webscraping}\n\nlibrary(rvest)\nlibrary(polite)\n\nurl <- \"https://fr.wikipedia.org/wiki/Liste_des_communes_de_la_Vend%C3%A9e\"\nsession <- polite::bow(url) \n# Bow permet d'interroger le robots.txt et nous informe du r√©sultat\n# il enregistre notamment le d√©lai minimum de requ√™tage autoris√© par le site\n\nsession\n\npage <- polite::scrape(session)\n# scrape() r√©cup√®re le contenu autoris√©\n\ncities_table <- page |> \n  rvest::html_element(\"table.wikitable\") |> \n  rvest::html_table()\n\n# html_element r√©cup√®re le premier √©l√©ment de la classe \"table.wikitable\"\n# pour tous les r√©cup√©rer sous forme de liste : html_elements()\n# html_table() permet de convertir des donn√©es tabulaires en un dataframe\n\nhead(cities_table)\n\n```\n\n```{r webscraping_2}\n#| eval: true\n#| echo: false\nhead(readr::read_csv(\"../../data/cities_table.csv\"))\n```\n\n# OCR et PDF  \n\nDans le domaine de la sant√©, les compte-rendus m√©dicaux contiennent √©norm√©ment de donn√©es int√©ressantes, mais sont malheureusement difficilement exploitables car dans un format non-structur√© : un papier scann√© et stock√© en PDF.\n\nL'OCR (Optical Character Recognition) est un syst√®me de *machine learning* permettant d'extraire le texte pr√©sent sur une image, parfait donc pour exploiter un grand nombre de comtpe-rendus scann√©s sans avoir √† le faire manuellement.\n\nDans cet exemple, nous utiliserons le mod√®le **Tesseract**, d√©velopp√© par Google et aujourd'hui Open Source. Nous utiliserons √©galement la librairie `doParallel` pour parall√©liser le traitement des images et augmenter la vitesse d'OCRisation\n\n```{r ocr}\nlibrary(tidyverse)\nlibrary(tesseract)\nlibrary(doParallel)\nlibrary(pdftools)\n\n# R√©cup√©rer le chemin de tous les fichiers PDF pr√©sents dans un dossier\nlist_pdf <- list.files(path = \"path/to/folder\", \n                       pattern = \"\\\\.pdf$\", \n                       all.files = TRUE, \n                       full.names = TRUE,\n                       recursive = FALSE\n                       )\n\nncores <- doParallel::detectCores(logical = FALSE) # compter les coeurs physiques \n\n# Cr√©er le cluster, en laissant 2 coeurs non utilis√©s\ncl <- doParallel::makeCluster(ncores-2)   \n# et pour arr√™ter : stopCluster(cl)\n\n\n# Charger les librariries et les objets dans les clusters\n## clusterEvalQ() : ex√©cute le code dans le cluster\nclusterEvalQ(cl, {\n  library(tidyverse);\n  library(tesseract);\n  library(pdftools)\n  })\n\n## clusterExport() charge l'objet dans le cluster\nclusterExport(cl, c(\"list_pdf\"))\n\n\n# Cr√©er une fonction qui renvoie un dataframe en sortie (1 ligne par page)\n\nmyPdfConvert <- function(list_objet) { \n  list_objet %>% map_df(~ data.frame(doc_origin = .x, #.x = chaque √©l√©ment de listobjet\n                                     texte = ocr(pdftools::pdf_convert(.x, dpi = 200)) # oc√©rise un pdf converti en png\n                                     ))\n}\n\n\n# Appliquer la fonction myPdfConvert √† chaque √©l√©ment de list_pdf\nres <- parLapply(cl, list_pdf, myPdfConvert)\n# Le r√©sultat est obtenu sous forme de liste (1 √©l√©ment par cluster) => les recombiner\nres <- do.call(\"rbind\", res)\n\n# Arr√™ter le cluster\nstopCluster(cl)\n\n\n# Pour r√©cup√©rer num√©ro de page et nom du doc √† chaque ligne (1 ligne = 1 page)\nresr <- res %>% \n  group_by(doc_origin) %>% \n  mutate(doc_page = 1:n(), doc_nom = str_extract_all(doc_origin, \"[[:alnum:]_ ]*\\\\.pdf\")) %>%\n  ungroup()\n\n\n```\n\n\nPour visualiser l'impact de la parall√©lisation : \n\n```{r ocr2}\n#| echo: false\n#| eval: true\n\nlibrary(ggplot2)\n\nplot <- data.frame(\n  time_s = c(16,34,49,79,140,28,95,148,340,445),\n  cluster = c(rep(\"Parall√©lis√©\",5), rep(\"Normal\",5)),\n  nombre_pages = c(24,60,108,216,300,24,60,108,216,300)\n  )\n\nggplot(plot,\n       aes(x = nombre_pages, \n           y = time_s, \n           group = cluster, \n           color = cluster)\n       ) +\n  geom_line() +\n  geom_point() +\n  ylab(\"Temps en secondes\") +\n  scale_x_continuous(\"Nombre de pages\", limits = c(0,300)) +\n  ggtitle(\"Comparaison du temps d'OCR selon le mode de calcul\")\n\n```\n\n\n\n\n\n# JSON et XML  \n\nLes formats **XML** (eXtensible Markup Language) et **JSON** (JavaScript Object Notation) sont largement utilis√©s pour le stockage et l‚Äô√©change de donn√©es, notamment dans les **API REST**, les **flux RSS**, ou le stockage de donn√©es hi√©rarchiques .\n\nPour lire les fichiers XML, on utilisera`{xml2}` et pour JSON :`{jsonlite}` (il en existe de nombreuses autres, mais ce sont les plus connues)\n\n## XML  \n\nSupposons que nous ayons un fichier \"livres.xml\" ressemblant √† cela :\n\n```{r, xml_1}\n#| echo: TRUE\n#| eval: TRUE\nlibrary(xml2)\n\nxml_doc <- as_xml_document(\n\"<livres>\n  <livre>\n    <titre>R pour les d√©butants</titre>\n    <auteur>Jean Martin</auteur>\n    <annee>2020</annee>\n  </livre>\n  <livre>\n    <titre>Analyse de donn√©es avec R</titre>\n    <auteur>Marie Dupont</auteur>\n    <annee>2021</annee>\n  </livre>\n</livres>\"\n)\n\n```\n\nPour lire le fichier .xml, on utilisera\n\n```{r, xml_2}\n#| echo: TRUE\n#| eval: FALSE\n\nxml_doc <- xml2::read_xml(\"livres.xml\")\n```\n\nEnsuite, on peut extraire les titres avec\n\n```{r xml_3}\n#| echo: TRUE\n#| eval: TRUE\n\ntitres_xml <- xml_find_all(xml_doc, \".//livre/titre\")\nprint(titres_xml)\n```\n\nLes fonctions `xml_find_*()` permettent d'utiliser des expression **xpath**, similaire √† du **REGEX** mais pour les architectures en arbre et renvoient des objets de classe `xml_nodeset`. Pour r√©cup√©rer un vecteur `character`, on rajoute simplement `xml_text()`\n\n```{r xml_4}\n#| echo: TRUE\n#| eval: TRUE\ntitres <- xml_text(titres_xml)\nprint(titres)\n```\n\n\n## JSON  \n\nPour lire du JSON, on utilisera simplement `fromJSON()`, pour convertir une cha√Æne de texte, ou `read_json()` pour lire un fichier .json. Les deux fonctions renvoient directement un `data.frame`\n\n```{r json_1}\n#| echo: TRUE\n#| eval: TRUE\n#| results: \"asis\"\nlibrary(jsonlite)\n\nlivres_json <- fromJSON(\n'\n[\n  {\n    \"titre\": \"R pour les d√©butants\",\n    \"auteur\": \"Jean Dupont\",\n    \"annee\": 2020\n  },\n  {\n    \"titre\": \"Analyse de donn√©es avec R\",\n    \"auteur\": \"Marie Curie\",\n    \"annee\": 2021\n  }\n]\n')\n\nprint(livres_json)\n```\n\nOn citera aussi la possibilit√© de convertir un objet R en JSON avec la fonction `toJSON()`. \n\n\n\n# DICOM  \n\nSans rentrer dans les d√©tails ici, le format DICOM (Digital Imaging and Communications in Medicine) est un standard pour l'imagerie m√©dicale. le package `{Espadon}` [d√©velopp√© par le CNRS](https://espadon.cnrs.fr/) permet d'interagir avec ce format\n\n# GPX  \n\nGPX est un format pour les donn√©es g√©ospatiales, qui permet par exemple de cr√©er un itin√©raire sur une carte. [Cet article](https://www.appsilon.com/post/r-gpx-files) explique tr√®s bien comment lire ce standard, souvent stock√© au format XML.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","srcMarkdownNoYaml":"\n\n\nR est un langage qui permet d'int√©ragir avec √† peu pr√®s n'importe quel format de donn√©es : \\n\n\n-   Des fichiers tabulaires (CSV, Excel, Parquet...), en local ou sur un serveur\n\n-   Des bases SQL/NoSQL : MS SQL Server, MySQL, MongoDB, Oracle, DuckDB...\n\n-   Des donn√©es h√©berg√©es sur un cloud (GCP, AWS, Azure...)\n\n-   Des donn√©es r√©cup√©r√©es via du webscraping ou une requ√™te API\n\n-   Des donn√©es au format JSON ou XML\n\nMais pas seulement ! R peut √©galement travailler avec des images (OCRiser un PDF pour r√©cup√©rer son contenu, ou en g√©n√©rer un), des fichiers DICOM, et bien plus encore.\n\n# CSV en local\n\nCommen√ßons avec le plus simple : les fichiers CSV\n\n```{r csv}\nlibrary(readr) # package du Tidyverse\nread_csv(\"path/to/iris.csv\")\n```\n\n\n```{r}\n#| eval: true\n#| echo: false\nhead(iris)\n```\n\n\n\nOn fait difficilement plus facile. On notera tout de m√™me que `{readr}` offre des arguments et fonctions suppl√©mentaires pour g√©rer diff√©rents probl√®mes que l'on rencontre souvent avec le CSV : les s√©parateurs et l'encodage. \\n\n\nLa fonction `read_csv` lit par d√©faut des fichiers au \"format international\" dont le s√©parateur est une virgule (,) et la d√©cimale un point (.). Cependant, en France (et en Europe), on utilis√© g√©n√©ralement la virgule comme s√©parateur d√©cimal. On a donc invent√© le format csv avec s√©parateur point-virgule (;) et d√©cimale en virgule (,). Pour lire un fichier sous ce format, on peut utiliser la fonction `read_csv2()` de `{readr}`. \\n\n\nPour des formats encore plus exotiques (tsv par exemple), `read_delim()` permet de pr√©ciser le d√©limiteur. \\n\n\nOn notera aussi que les fonctions `read_*()` fournissent des arguments permettant d'expliciter les valeurs nulles, de retirer les espaces en d√©but/fin de cha√Æne (ce qui arrive tr√®s souvent sur des donn√©es saisies √† la main dans Excel), de diff√©rencier des noms de colonnes en doublons, ou encore de sauter les premi√®res lignes d'un fichier.\n\n```{r csv_2}\nlibrary(readr)\nread_csv2(\"path/to/file_fr.csv\", # lire un fichier au format FR, delim = ;\n          na = c(\"\",\"NULL\",\"NA\"), # les cases contenant \"\",\"NULL\" et \"NA\" seront vides\n          trim_ws = TRUE, # trim_whitespace : retirer les espaces en fin de cha√Æne\n          skip = 1, # sauter la premi√®re ligne\n          name_repair = \"unique\" # diff√©rencier les noms en doublons\n)\n```\n\n# CSV √† distance\n\nLes fonctions `read.*()` (en base R) et `read_*()` (`{readr}`) permettent toutes de charger des fichiers CSV via une URL.\n\n```{r read_csv_url}\n\ncovid_data <- readr::read_csv2(\"https://www.data.gouv.fr/api/1/datasets/r/fe3e7099-a975-4181-9fb5-2dd1b8f1b552\")\nhead(covid_data)\n```\n\n```{r read_csv_url_2}\n#| eval: true\n#| echo: false\n\nhead(readr::read_csv(\"../../data/covid_data.csv\"))\n```\n\n\n\n# Excel\n\nLes fichiers dits \"Excel\" ont l'extension .xls ou .xlsx, et peuvent contenir diff√©rents feuillets, de la mise en forme, des cellules fusionn√©es, des formules, des commentaires et des graphes. \\n\n\nPour lire ces donn√©es, il existe plusieurs librairies : `{readxl}`, `{openxslx}` ou `{openxslx2}`\n\n```{r readxl}\n\nlibrary(readxl)\nread_excel(\"path/to/file.xslx\",\n           sheet = 1, # feuillet, par num√©ro ou par nom\n           range = \"B3:D87\", # les cellules √† garder (optionnel)\n           na = c(\"\",\"NULL\",\"NA\"), # les cellules √† consid√©rer vides\n           .name_repair = \"unique\" # diff√©rencier les noms en doublons\n)\n\n```\n\n# Parquet\n\nLe format .parquet, encore malheureusement peu connu, est un format compress√© et orient√© en colonnes, orient√© vers la performance. Un m√™me jeu de donn√©es au format parquet est entre 5 et 10 fois moins volumineux qu'en csv et est optimis√© pour √™tre lu rapidement par R ou Python. On citera aussi sa compatibilit√© avec DuckDB pour former un duo parfait pour [travailler le Big Data avec R](decouvrir_bigdata.qmd). L'un des rares d√©fauts qu'on peut lui trouver est de ne pas √™tre compatible avec Excel, ce qui limite sa diffusion. \\n\n\nSi vous ne l'avez pas encore lu, je vous renvoie vers cet excellent article : [Parquet devrait remplacer le format CSV](https://www.icem7.fr/cartographie/parquet-devrait-remplacer-le-format-csv/)\n\nIl existe deux m√©thodes principales pour lire un fichier parquet en R : `{arrow}`, la librairie de [Apache Arrow](https://arrow.apache.org/) et `{duckDB}`, la librairie de [DuckDB](https://duckdb.org/).\n\nLes deux m√©thodes permettent de cr√©er une *connexion* vers le jeu de donn√©es, au lieu de les importer dans la m√©moire de R, permettant de travailler avec des donn√©es pus volumineuses que la RAM. Les deux m√©thodes permettent d'utiliser `{dplyr}` pour le requ√™tage, mais `{duckdb}` permet aussi de requ√™ter n'importe quel fichier plat en SQL. \\n\n\nOn notera √©galement la capacit√© √† lire plusieurs fichiers regroup√©s dans un dossier (ayant le m√™me sch√©ma), voire m√™me des fichiers partitionn√©s ([au style Hive](https://arrow.apache.org/docs/r/reference/hive_partition.html)), permettant de ne scanner que les donn√©es n√©cessaires, pour optimiser encore plus la performance de requ√™tage.\n\n```{r parquet}\n\n# Arrow : \nlibrary(arrow)\narrow_df <- read_parquet(\"path/to/file.parquet\")\n\n# Il est possible de lire directement un ensemble de fichiers .parquet ayant le m√™me sch√©ma \n# et regroup√©s dans un dossier (donn√©es par batch, avec un fichier par mois par exemple)\narrow_dataset <- open_dataset(\"path/to/folder\")\n# Ici, arrow_dataset est une connexion, de classe `Dataset`\n# Un `Dataset` de {arrow} se requ√™te avec {dplyr}\n# pour r√©cup√©rer les donn√©es dans l'environnement de R : \narrow_df <- arrow_dataset |> \n  filter(...) |>  # la plupart des fonctions de {dplyr} sont compatibles\n  select(...) |>  \n  collect() # pour ex√©cuter la requ√™te et r√©cup√©rer un tibble/dataframe\n\n\n\n# DuckDB :\nlibrary(duckdb)\nlibrary(DBI) # DataBase Interface\nlibrary(dplyr)\n\nconn_ddb <- DBI::dbConnect(duckdb(), dbdir = \":memory:\") # Cr√©er une base duckdb virtuelle\nduck_df <- conn_ddb |> \n  tbl(\"path/dataset/**/*.parquet\") |> \n  filter(...)\n# Le ** signifie \"tout\", et permet de lire tous les fichiers .parquet du dossier\n# Dans un fichier partitionn√© par exemple\n\n```\n\n\n:::{.callout-tip}\n## Note  \n{arrow} renvoie des objets de type `arrow_table` ou `Dataset`, qui ne sont compatibles qu'avec les fonctions de `{dplyr}`. Si vous souhaitez modifier une colonne avec une fonction de `{purrr}`, `{lubridate}` ou `{stringr}`, il faudra d'abord utiliser `collect()` pour obtenir un `tibble`. \\n\n\nA l'inverse, `{duckdb}` renvoie des objets de classe `tbl` (donc des `tibble`). Si l'on ajoute l'incroyable performance de requ√™tage du moteur duckdb, j'aurais plut√¥t tendance √† privil√©gier cette m√©thode\n:::\n\n:::{.callout-important}\n## Info  \nR√©cemment, `{duckplyr}`, une librairie int√©grant `{dplyr}` avec le moteur de `{duckdb}` [a rejoint le `Tidyverse`](https://www.tidyverse.org/blog/2025/06/duckplyr-1-1-0/), signe que Duckdb est per√ßu comme un outil d'avenir.\n\n:::\n\n\n# Connexions aux bases SQL\n\nSi vous utilisez Rstudio, le moyen le plus simple de se connecter √† une source de donn√©es est d'utiliser l'onglet *Connections*, g√©n√©ralement situ√© en haut √† droite, avec votre *Environnement*. En cliquant sur *New connection*, une fen√™tre appara√Æt et va automatiquement vous proposer les sources √† disposition. \\n\n\n![Exemple des connexions existantes sur mon poste](/images/rstudio_connections.png)\n\nSi vous avez d√©j√† configur√© un DSN contenant vos identifiants, les informations de connexion devraient d√©j√† √™tre remplies, vous n'avez plus qu'√† cliquer sur \"OK\", et le code s'ex√©cutera dans la console. Vous pourrez alors visualiser vos bases dans l'onglet *Connections*\n\n![](/images/rstudio_connections_dsn.png){width=\"515\"}\n\nSinon, vous aurez besoin d'entrer les param√®tres suivants :\n\n-   *user* = \"...\" pour le nom d'utilisateur\n\n-   *password* = \"...\" pour le mot de passe\n\n-   *host* et *port* si besoin\n\n-   *dbname* pour le nom de la base de donn√©es\n\nSi vous utilisez uniquement la console, vous pouvez rentrer directement les param√®tres dans `dbConnect()`, en arguments de la fonction.\\n\n\nUne fois connect√©, on peut requ√™ter ses donn√©es avec `{dplyr}` ou en SQL, en utilisant `dbGetQuery()`\n\n```{r db_get_query}\nlibrary(DBI)\nlibrary(odbc)\nconn <- DBI::dbConnect(odbc::odbc() ,\"server-name\", database = \"MA_BASE\")\n\n# SQL\nma_table <- DBI::dbGetQuery(conn, \n                            \"SELECT * FROM MA_TABLE\n                            WHERE ...\")\n\n\n# DPLYR\nlibrary(dplyr)\nma_table_2 <- dplyr::tbl(conn, \"MA_TABLE\") |>  # ici, on cr√©√© une connexion √† la table\n  dplyr::filter(...) |>  # on cr√©√© la requ√™te\n  dplyr::collect() # et on collecte le r√©sultat dans R avec collect()\n```\n\nPour interagir avec une base SQL pour autre chose qu'une requ√™te (UPDATE, DROP, ...), on peut utiliser la librairie `{dbplyr}`, un back-end de `{dplyr}` pour les bases de donn√©es, ou utiliser `DBI::dbExecute()`\n\n# Cloud\n\nR dispose de nombreuses librairies pour r√©cup√©rer des donn√©es h√©berg√©es sur un serveur cloud. \\n Je n'entrerai pas dans les d√©tails de cette partie, car je n'ai pas encore eu beaucoup l'occasion de pratiquer par moi-m√™me, mais je penserai √† la mettre √† jour d√®s que possible. \\n\n\n## Google Cloud Platform\n\nVoici une liste non exhaustive des librairies permettant de travailler avec Google Cloud Platform, disponible sur la vignette de [`{googleAuthR}`](https://code.markedmondson.me/googleAuthR/)\n\n-   [googleComputeEngineR](https://cloudyr.github.io/googleComputeEngineR/) - Google Compute Engine VMs API\n\n-   [searchConsoleR](https://code.markedmondson.me/searchConsoleR/) - Search Console API\n\n-   [bigQueryR](https://code.markedmondson.me/bigQueryR/) - BigQuery API. Part of the cloudyr project.\n\n-   [googleAnalyticsR](https://code.markedmondson.me/googleAnalyticsR/) - Google Analytics API\n\n-   [googleTagManagerR](https://github.com/IronistM/googleTagManageR) - Google Tag Manager API by IronistM\n\n-   [googleID](https://github.com/MarkEdmondson1234/googleID) - Simple user info from G+ API for Shiny app authentication flows.\n\n-   [googleCloudStorageR](https://code.markedmondson.me/googleCloudStorageR/) - Google Cloud Storage API\n\n-   [RoogleVision](https://github.com/cloudyr/googleCloudVisionR) - R Package for Image Recogntion, Object Detection, and OCR using the Google‚Äôs Cloud Vision API\n\n-   [googleLanguageR](https://github.com/ropensci/googleLanguageR) - Access Speech to Text, Entity analysis and translation APIs from R\n\n-   [googleCloudRunner](https://code.markedmondson.me/googleCloudRunner/) - Continuous Development and Integration with Cloud Run, Cloud Scheduler and Cloud Build\n\n## AWS\n\nLa documentation officielle de Amazon Web Service dispose [d'un tutoriel](https://aws.amazon.com/fr/blogs/opensource/getting-started-with-r-on-amazon-web-services/) pour acc√©der aux donn√©es du service depuis R.\n\n# API REST\n\nLe plus simple selon moi pour requ√™ter une API REST avec R est d'utiliser l'excellent `{httr2}` de Hadley Wickham (que vous connaissez d√©j√† certainement si vous utilisez le Tidyverse). La documentation du package est disponible [ici](https://httr2.r-lib.org/)\n\nLa premi√®re chose √† faire est √©videmment de lire la documentation de l'API que l'on souhaite requ√™ter. Pour vous entra√Æner, vous pouvez utiliser [une des API disponibles sur data.gouv.fr](https://www.data.gouv.fr/dataservices). Pour cet exemple, j'utiliserai [l'API de Hub'eau pour la qualit√© de l'eau potable en France](https://hubeau.eaufrance.fr/page/api-qualite-eau-potable#console). \\n\n\nUne fois rendus sur le site, la documentation nous indique que la \"Base URL\" est hubeau.eaufrance.fr/api, puis nous indique les extensions √† ajouter selon ce que l'on souhaite requ√™ter (liste des communes ou r√©sultats), puis les parm√®tres de filtrage\n\nOn utilisera √©galement la librairie `{jsonlite}` pour convertir les donn√©es r√©cup√©r√©es au format JSON en un data.frame\n\n::: callout-tip\n## Astuce\nLe package `{httr2}` n√©cessite l'installation du package `{curl}`, qui peut poser probl√®me si vous √™tes sur un r√©seau professionnel prot√©g√© par un pare-feu. Dans ce cas, contactez votre DSI, ou ... faites un partage de connexion depuis votre t√©l√©phone ü§´\n:::\n\n```{r hubeau}\nlibrary(httr2)\nlibrary(jsonlite)\n\nbase_url <- \"http://hubeau.eaufrance.fr/api\"\n\nreq <- request(base_url) |>  # pointer vers l'url de base\n  req_url_path_append(\"/v1/qualite_eau_potable/resultats_dis\") |> #extension\n  req_url_query(code_departement = \"85\") |> # param√®tres de la requ√™te\n  req_url_query(size = \"50\") |> \n  req_url_query(    \n    fields = c('libelle_parametre','libelle_parametre_maj',\n               'resultat_numerique', 'libelle_unite',\n               'limite_qualite_parametre','reference_qualite_parametre',\n               'nom_commune','date_prelevement',\n               'conclusion_conformite_prelevement'), \n    .multi = \"comma\")\n\nresp <- req_perform(req)\n\ndf_data <- resp |>\n  resp_body_string() |>\n  fromJSON()\n\ndf_data <- df_data$data\n\nhead(df_data)\n\n```\n\n```{r hubeau_2}\n#| eval: true\n#| echo: false\n\nhead(readr::read_csv(\"../../data/hubeau_data.csv\"))\n```\n\n\n::: callout-tip\n## Cacher les cl√©s API\nDans le cas o√π vous utilisez une API priv√©e n√©cessitant une cl√©, il est judicieux d'√©viter de stocker vos identifiants dans le script R. Pour cela, vous pouvez utiliser un fichier [.Rprofile](https://docs.posit.co/ide/user/ide/guide/environments/r/managing-r.html), stock√© sur votre machine locale, et qui se lance quand vous d√©marrez votre IDE.\nAinsi, vos identifiants sont pr√©sents dans votre environnement sans avoir √† les d√©clarer dans le script. Si vous h√©bergez votre code sur GitHub, vous pouvez inclure le .Rprofile dans le .gitignore pour √©viter de les inclure dans le repo, et utiliser la fonction [*Github Secrets*](https://docs.github.com/fr/actions/how-tos/write-workflows/choose-what-workflows-do/use-secrets) pour remplacer le .Rprofile.\n:::\n\n\n\n# Webscraping  \n\nSi vous souhaitez r√©cup√©rer des donn√©es sur un site web qui ne propose pas d'API, l'une des solutions est le [*web scraping*](https://fr.wikipedia.org/wiki/Web_scraping). \n\n\n::: callout-warning\n## Attention\nTous les sites n'autorisent pas le web scraping, et les donn√©es doivent √™tre *nativement* publiques, *tomb√©es* dans le domaine publique ou sous licence libre *si votre usage n'est pas commercial*. Renseignez-vous avant de scraper une page !\n:::\n\nLe principal package de web scraping en R est [`{rvest}`](https://rvest.tidyverse.org/index.html) (toujours d√©velopp√© par Hadley Wickham). \nPour cet exemple, j'utiliserai aussi [`{polite}`](https://dmi3kno.github.io/polite/index.html), un package qui permet de suivre 3 r√®gles d'√©thique lors d'une session de scraping : *\"Demander la permission, prendre doucement et ne jamais demander deux fois\"*.\nCes r√®gles permettent d'√©viter de causer des probl√®mes en r√©coltant des donn√©es non autoris√©es ou en surchargeant le serveur de requ√™tes, et nous √©viterons d'√™tre bannis par les administrateurs du site scrap√©.\n\nDans cet exemple, nous allons r√©cup√©rer une table contenant la liste des communes de Vend√©e, sur [cette page wikip√©dia](https://fr.wikipedia.org/wiki/Liste_des_communes_de_la_Vend%C3%A9e)\n\n```{r webscraping}\n\nlibrary(rvest)\nlibrary(polite)\n\nurl <- \"https://fr.wikipedia.org/wiki/Liste_des_communes_de_la_Vend%C3%A9e\"\nsession <- polite::bow(url) \n# Bow permet d'interroger le robots.txt et nous informe du r√©sultat\n# il enregistre notamment le d√©lai minimum de requ√™tage autoris√© par le site\n\nsession\n\npage <- polite::scrape(session)\n# scrape() r√©cup√®re le contenu autoris√©\n\ncities_table <- page |> \n  rvest::html_element(\"table.wikitable\") |> \n  rvest::html_table()\n\n# html_element r√©cup√®re le premier √©l√©ment de la classe \"table.wikitable\"\n# pour tous les r√©cup√©rer sous forme de liste : html_elements()\n# html_table() permet de convertir des donn√©es tabulaires en un dataframe\n\nhead(cities_table)\n\n```\n\n```{r webscraping_2}\n#| eval: true\n#| echo: false\nhead(readr::read_csv(\"../../data/cities_table.csv\"))\n```\n\n# OCR et PDF  \n\nDans le domaine de la sant√©, les compte-rendus m√©dicaux contiennent √©norm√©ment de donn√©es int√©ressantes, mais sont malheureusement difficilement exploitables car dans un format non-structur√© : un papier scann√© et stock√© en PDF.\n\nL'OCR (Optical Character Recognition) est un syst√®me de *machine learning* permettant d'extraire le texte pr√©sent sur une image, parfait donc pour exploiter un grand nombre de comtpe-rendus scann√©s sans avoir √† le faire manuellement.\n\nDans cet exemple, nous utiliserons le mod√®le **Tesseract**, d√©velopp√© par Google et aujourd'hui Open Source. Nous utiliserons √©galement la librairie `doParallel` pour parall√©liser le traitement des images et augmenter la vitesse d'OCRisation\n\n```{r ocr}\nlibrary(tidyverse)\nlibrary(tesseract)\nlibrary(doParallel)\nlibrary(pdftools)\n\n# R√©cup√©rer le chemin de tous les fichiers PDF pr√©sents dans un dossier\nlist_pdf <- list.files(path = \"path/to/folder\", \n                       pattern = \"\\\\.pdf$\", \n                       all.files = TRUE, \n                       full.names = TRUE,\n                       recursive = FALSE\n                       )\n\nncores <- doParallel::detectCores(logical = FALSE) # compter les coeurs physiques \n\n# Cr√©er le cluster, en laissant 2 coeurs non utilis√©s\ncl <- doParallel::makeCluster(ncores-2)   \n# et pour arr√™ter : stopCluster(cl)\n\n\n# Charger les librariries et les objets dans les clusters\n## clusterEvalQ() : ex√©cute le code dans le cluster\nclusterEvalQ(cl, {\n  library(tidyverse);\n  library(tesseract);\n  library(pdftools)\n  })\n\n## clusterExport() charge l'objet dans le cluster\nclusterExport(cl, c(\"list_pdf\"))\n\n\n# Cr√©er une fonction qui renvoie un dataframe en sortie (1 ligne par page)\n\nmyPdfConvert <- function(list_objet) { \n  list_objet %>% map_df(~ data.frame(doc_origin = .x, #.x = chaque √©l√©ment de listobjet\n                                     texte = ocr(pdftools::pdf_convert(.x, dpi = 200)) # oc√©rise un pdf converti en png\n                                     ))\n}\n\n\n# Appliquer la fonction myPdfConvert √† chaque √©l√©ment de list_pdf\nres <- parLapply(cl, list_pdf, myPdfConvert)\n# Le r√©sultat est obtenu sous forme de liste (1 √©l√©ment par cluster) => les recombiner\nres <- do.call(\"rbind\", res)\n\n# Arr√™ter le cluster\nstopCluster(cl)\n\n\n# Pour r√©cup√©rer num√©ro de page et nom du doc √† chaque ligne (1 ligne = 1 page)\nresr <- res %>% \n  group_by(doc_origin) %>% \n  mutate(doc_page = 1:n(), doc_nom = str_extract_all(doc_origin, \"[[:alnum:]_ ]*\\\\.pdf\")) %>%\n  ungroup()\n\n\n```\n\n\nPour visualiser l'impact de la parall√©lisation : \n\n```{r ocr2}\n#| echo: false\n#| eval: true\n\nlibrary(ggplot2)\n\nplot <- data.frame(\n  time_s = c(16,34,49,79,140,28,95,148,340,445),\n  cluster = c(rep(\"Parall√©lis√©\",5), rep(\"Normal\",5)),\n  nombre_pages = c(24,60,108,216,300,24,60,108,216,300)\n  )\n\nggplot(plot,\n       aes(x = nombre_pages, \n           y = time_s, \n           group = cluster, \n           color = cluster)\n       ) +\n  geom_line() +\n  geom_point() +\n  ylab(\"Temps en secondes\") +\n  scale_x_continuous(\"Nombre de pages\", limits = c(0,300)) +\n  ggtitle(\"Comparaison du temps d'OCR selon le mode de calcul\")\n\n```\n\n\n\n\n\n# JSON et XML  \n\nLes formats **XML** (eXtensible Markup Language) et **JSON** (JavaScript Object Notation) sont largement utilis√©s pour le stockage et l‚Äô√©change de donn√©es, notamment dans les **API REST**, les **flux RSS**, ou le stockage de donn√©es hi√©rarchiques .\n\nPour lire les fichiers XML, on utilisera`{xml2}` et pour JSON :`{jsonlite}` (il en existe de nombreuses autres, mais ce sont les plus connues)\n\n## XML  \n\nSupposons que nous ayons un fichier \"livres.xml\" ressemblant √† cela :\n\n```{r, xml_1}\n#| echo: TRUE\n#| eval: TRUE\nlibrary(xml2)\n\nxml_doc <- as_xml_document(\n\"<livres>\n  <livre>\n    <titre>R pour les d√©butants</titre>\n    <auteur>Jean Martin</auteur>\n    <annee>2020</annee>\n  </livre>\n  <livre>\n    <titre>Analyse de donn√©es avec R</titre>\n    <auteur>Marie Dupont</auteur>\n    <annee>2021</annee>\n  </livre>\n</livres>\"\n)\n\n```\n\nPour lire le fichier .xml, on utilisera\n\n```{r, xml_2}\n#| echo: TRUE\n#| eval: FALSE\n\nxml_doc <- xml2::read_xml(\"livres.xml\")\n```\n\nEnsuite, on peut extraire les titres avec\n\n```{r xml_3}\n#| echo: TRUE\n#| eval: TRUE\n\ntitres_xml <- xml_find_all(xml_doc, \".//livre/titre\")\nprint(titres_xml)\n```\n\nLes fonctions `xml_find_*()` permettent d'utiliser des expression **xpath**, similaire √† du **REGEX** mais pour les architectures en arbre et renvoient des objets de classe `xml_nodeset`. Pour r√©cup√©rer un vecteur `character`, on rajoute simplement `xml_text()`\n\n```{r xml_4}\n#| echo: TRUE\n#| eval: TRUE\ntitres <- xml_text(titres_xml)\nprint(titres)\n```\n\n\n## JSON  \n\nPour lire du JSON, on utilisera simplement `fromJSON()`, pour convertir une cha√Æne de texte, ou `read_json()` pour lire un fichier .json. Les deux fonctions renvoient directement un `data.frame`\n\n```{r json_1}\n#| echo: TRUE\n#| eval: TRUE\n#| results: \"asis\"\nlibrary(jsonlite)\n\nlivres_json <- fromJSON(\n'\n[\n  {\n    \"titre\": \"R pour les d√©butants\",\n    \"auteur\": \"Jean Dupont\",\n    \"annee\": 2020\n  },\n  {\n    \"titre\": \"Analyse de donn√©es avec R\",\n    \"auteur\": \"Marie Curie\",\n    \"annee\": 2021\n  }\n]\n')\n\nprint(livres_json)\n```\n\nOn citera aussi la possibilit√© de convertir un objet R en JSON avec la fonction `toJSON()`. \n\n\n\n# DICOM  \n\nSans rentrer dans les d√©tails ici, le format DICOM (Digital Imaging and Communications in Medicine) est un standard pour l'imagerie m√©dicale. le package `{Espadon}` [d√©velopp√© par le CNRS](https://espadon.cnrs.fr/) permet d'interagir avec ce format\n\n# GPX  \n\nGPX est un format pour les donn√©es g√©ospatiales, qui permet par exemple de cr√©er un itin√©raire sur une carte. [Cet article](https://www.appsilon.com/post/r-gpx-files) explique tr√®s bien comment lire ce standard, souvent stock√© au format XML.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"decouvrir_connections.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","editor":"source","theme":"sandstone","title":"Recueil de donn√©es","description":"Se connecter √† tout type de donn√©es","author":"F√©lix Marchais","date":"2025-07-29","categories":["R","Bases de donn√©es"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}