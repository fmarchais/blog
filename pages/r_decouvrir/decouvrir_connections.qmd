---
title: "Recueil de donn√©es"
description: "Se connecter √† tout type de donn√©es"
format: html
author: "F√©lix Marchais"
date: "2025-07-29"
categories:
  - R
  - Bases de donn√©es
execute: 
  eval: false 
  echo: true
  message: false
  warning: false
---


R est un langage qui permet d'int√©ragir avec √† peu pr√®s n'importe quel format de donn√©es : \n

-   Des fichiers tabulaires (CSV, Excel, Parquet...), en local ou sur un serveur

-   Des bases SQL/NoSQL : MS SQL Server, MySQL, MongoDB, Oracle, DuckDB...

-   Des donn√©es h√©berg√©es sur un cloud (GCP, AWS, Azure...)

-   Des donn√©es r√©cup√©r√©es via du webscraping ou une requ√™te API

-   Des donn√©es au format JSON ou XML

Mais pas seulement ! R peut √©galement travailler avec des images (OCRiser un PDF pour r√©cup√©rer son contenu, ou en g√©n√©rer un), des fichiers DICOM, et bien plus encore.

# CSV en local

Commen√ßons avec le plus simple : les fichiers CSV

```{r csv}
library(readr) # package du Tidyverse
read_csv("path/to/iris.csv")
```


```{r}
#| eval: true
#| echo: false
head(iris)
```



On fait difficilement plus facile. On notera tout de m√™me que `{readr}` offre des arguments et fonctions suppl√©mentaires pour g√©rer diff√©rents probl√®mes que l'on rencontre souvent avec le CSV : les s√©parateurs et l'encodage. \n

La fonction `read_csv` lit par d√©faut des fichiers au "format international" dont le s√©parateur est une virgule (,) et la d√©cimale un point (.). Cependant, en France (et en Europe), on utilis√© g√©n√©ralement la virgule comme s√©parateur d√©cimal. On a donc invent√© le format csv avec s√©parateur point-virgule (;) et d√©cimale en virgule (,). Pour lire un fichier sous ce format, on peut utiliser la fonction `read_csv2()` de `{readr}`. \n

Pour des formats encore plus exotiques (tsv par exemple), `read_delim()` permet de pr√©ciser le d√©limiteur. \n

On notera aussi que les fonctions `read_*()` fournissent des arguments permettant d'expliciter les valeurs nulles, de retirer les espaces en d√©but/fin de cha√Æne (ce qui arrive tr√®s souvent sur des donn√©es saisies √† la main dans Excel), de diff√©rencier des noms de colonnes en doublons, ou encore de sauter les premi√®res lignes d'un fichier.

```{r csv_2}
library(readr)
read_csv2("path/to/file_fr.csv", # lire un fichier au format FR, delim = ;
          na = c("","NULL","NA"), # les cases contenant "","NULL" et "NA" seront vides
          trim_ws = TRUE, # trim_whitespace : retirer les espaces en fin de cha√Æne
          skip = 1, # sauter la premi√®re ligne
          name_repair = "unique" # diff√©rencier les noms en doublons
)
```

# CSV √† distance

Les fonctions `read.*()` (en base R) et `read_*()` (`{readr}`) permettent toutes de charger des fichiers CSV via une URL.

```{r read_csv_url}

covid_data <- readr::read_csv2("https://www.data.gouv.fr/api/1/datasets/r/fe3e7099-a975-4181-9fb5-2dd1b8f1b552")
head(covid_data)
```

```{r read_csv_url_2}
#| eval: true
#| echo: false

head(readr::read_csv("../../data/covid_data.csv"))
```



# Excel

Les fichiers dits "Excel" ont l'extension .xls ou .xlsx, et peuvent contenir diff√©rents feuillets, de la mise en forme, des cellules fusionn√©es, des formules, des commentaires et des graphes. \n

Pour lire ces donn√©es, il existe plusieurs librairies : `{readxl}`, `{openxslx}` ou `{openxslx2}`

```{r readxl}

library(readxl)
read_excel("path/to/file.xslx",
           sheet = 1, # feuillet, par num√©ro ou par nom
           range = "B3:D87", # les cellules √† garder (optionnel)
           na = c("","NULL","NA"), # les cellules √† consid√©rer vides
           .name_repair = "unique" # diff√©rencier les noms en doublons
)

```

# Parquet

Le format .parquet, encore malheureusement peu connu, est un format compress√© et orient√© en colonnes, orient√© vers la performance. Un m√™me jeu de donn√©es au format parquet est entre 5 et 10 fois moins volumineux qu'en csv et est optimis√© pour √™tre lu rapidement par R ou Python. On citera aussi sa compatibilit√© avec DuckDB pour former un duo parfait pour [travailler le Big Data avec R](decouvrir_bigdata.qmd). L'un des rares d√©fauts qu'on peut lui trouver est de ne pas √™tre compatible avec Excel, ce qui limite sa diffusion. \n

Si vous ne l'avez pas encore lu, je vous renvoie vers cet excellent article : [Parquet devrait remplacer le format CSV](https://www.icem7.fr/cartographie/parquet-devrait-remplacer-le-format-csv/)

Il existe deux m√©thodes principales pour lire un fichier parquet en R : `{arrow}`, la librairie de [Apache Arrow](https://arrow.apache.org/) et `{duckDB}`, la librairie de [DuckDB](https://duckdb.org/).

Les deux m√©thodes permettent de cr√©er une *connexion* vers le jeu de donn√©es, au lieu de les importer dans la m√©moire de R, permettant de travailler avec des donn√©es pus volumineuses que la RAM. Les deux m√©thodes permettent d'utiliser `{dplyr}` pour le requ√™tage, mais `{duckdb}` permet aussi de requ√™ter n'importe quel fichier plat en SQL. \n

On notera √©galement la capacit√© √† lire plusieurs fichiers regroup√©s dans un dossier (ayant le m√™me sch√©ma), voire m√™me des fichiers partitionn√©s ([au style Hive](https://arrow.apache.org/docs/r/reference/hive_partition.html)), permettant de ne scanner que les donn√©es n√©cessaires, pour optimiser encore plus la performance de requ√™tage.

```{r parquet}

# Arrow : 
library(arrow)
arrow_df <- read_parquet("path/to/file.parquet")

# Il est possible de lire directement un ensemble de fichiers .parquet ayant le m√™me sch√©ma 
# et regroup√©s dans un dossier (donn√©es par batch, avec un fichier par mois par exemple)
arrow_dataset <- open_dataset("path/to/folder")
# Ici, arrow_dataset est une connexion, de classe `Dataset`
# Un `Dataset` de {arrow} se requ√™te avec {dplyr}
# pour r√©cup√©rer les donn√©es dans l'environnement de R : 
arrow_df <- arrow_dataset |> 
  filter(...) |>  # la plupart des fonctions de {dplyr} sont compatibles
  select(...) |>  
  collect() # pour ex√©cuter la requ√™te et r√©cup√©rer un tibble/dataframe



# DuckDB :
library(duckdb)
library(DBI) # DataBase Interface
library(dplyr)

conn_ddb <- DBI::dbConnect(duckdb(), dbdir = ":memory:") # Cr√©er une base duckdb virtuelle
duck_df <- conn_ddb |> 
  tbl("path/dataset/**/*.parquet") |> 
  filter(...)
# Le ** signifie "tout", et permet de lire tous les fichiers .parquet du dossier
# Dans un fichier partitionn√© par exemple

```


:::{.callout-tip}
## Note  
{arrow} renvoie des objets de type `arrow_table` ou `Dataset`, qui ne sont compatibles qu'avec les fonctions de `{dplyr}`. Si vous souhaitez modifier une colonne avec une fonction de `{purrr}`, `{lubridate}` ou `{stringr}`, il faudra d'abord utiliser `collect()` pour obtenir un `tibble`. \n

A l'inverse, `{duckdb}` renvoie des objets de classe `tbl` (donc des `tibble`). Si l'on ajoute l'incroyable performance de requ√™tage du moteur duckdb, j'aurais plut√¥t tendance √† privil√©gier cette m√©thode
:::

:::{.callout-important}
## Info  
R√©cemment, `{duckplyr}`, une librairie int√©grant `{dplyr}` avec le moteur de `{duckdb}` [a rejoint le `Tidyverse`](https://www.tidyverse.org/blog/2025/06/duckplyr-1-1-0/), signe que Duckdb est per√ßu comme un outil d'avenir.

:::


# Connexions aux bases SQL

Si vous utilisez Rstudio, le moyen le plus simple de se connecter √† une source de donn√©es est d'utiliser l'onglet *Connections*, g√©n√©ralement situ√© en haut √† droite, avec votre *Environnement*. En cliquant sur *New connection*, une fen√™tre appara√Æt et va automatiquement vous proposer les sources √† disposition. \n

![Exemple des connexions existantes sur mon poste](/images/rstudio_connections.png)

Si vous avez d√©j√† configur√© un DSN contenant vos identifiants, les informations de connexion devraient d√©j√† √™tre remplies, vous n'avez plus qu'√† cliquer sur "OK", et le code s'ex√©cutera dans la console. Vous pourrez alors visualiser vos bases dans l'onglet *Connections*

![](/images/rstudio_connections_dsn.png){width="515"}

Sinon, vous aurez besoin d'entrer les param√®tres suivants :

-   *user* = "..." pour le nom d'utilisateur

-   *password* = "..." pour le mot de passe

-   *host* et *port* si besoin

-   *dbname* pour le nom de la base de donn√©es

Si vous utilisez uniquement la console, vous pouvez rentrer directement les param√®tres dans `dbConnect()`, en arguments de la fonction.\n

Une fois connect√©, on peut requ√™ter ses donn√©es avec `{dplyr}` ou en SQL, en utilisant `dbGetQuery()`

```{r db_get_query}
library(DBI)
library(odbc)
conn <- DBI::dbConnect(odbc::odbc() ,"server-name", database = "MA_BASE")

# SQL
ma_table <- DBI::dbGetQuery(conn, 
                            "SELECT * FROM MA_TABLE
                            WHERE ...")


# DPLYR
library(dplyr)
ma_table_2 <- dplyr::tbl(conn, "MA_TABLE") |>  # ici, on cr√©√© une connexion √† la table
  dplyr::filter(...) |>  # on cr√©√© la requ√™te
  dplyr::collect() # et on collecte le r√©sultat dans R avec collect()
```

Pour interagir avec une base SQL pour autre chose qu'une requ√™te (UPDATE, DROP, ...), on peut utiliser la librairie `{dbplyr}`, un back-end de `{dplyr}` pour les bases de donn√©es, ou utiliser `DBI::dbExecute()`

# Cloud

R dispose de nombreuses librairies pour r√©cup√©rer des donn√©es h√©berg√©es sur un serveur cloud. \n Je n'entrerai pas dans les d√©tails de cette partie, car je n'ai pas encore eu beaucoup l'occasion de pratiquer par moi-m√™me, mais je penserai √† la mettre √† jour d√®s que possible. \n

## Google Cloud Platform

Voici une liste non exhaustive des librairies permettant de travailler avec Google Cloud Platform, disponible sur la vignette de [`{googleAuthR}`](https://code.markedmondson.me/googleAuthR/)

-   [googleComputeEngineR](https://cloudyr.github.io/googleComputeEngineR/) - Google Compute Engine VMs API

-   [searchConsoleR](https://code.markedmondson.me/searchConsoleR/) - Search Console API

-   [bigQueryR](https://code.markedmondson.me/bigQueryR/) - BigQuery API. Part of the cloudyr project.

-   [googleAnalyticsR](https://code.markedmondson.me/googleAnalyticsR/) - Google Analytics API

-   [googleTagManagerR](https://github.com/IronistM/googleTagManageR) - Google Tag Manager API by IronistM

-   [googleID](https://github.com/MarkEdmondson1234/googleID) - Simple user info from G+ API for Shiny app authentication flows.

-   [googleCloudStorageR](https://code.markedmondson.me/googleCloudStorageR/) - Google Cloud Storage API

-   [RoogleVision](https://github.com/cloudyr/googleCloudVisionR) - R Package for Image Recogntion, Object Detection, and OCR using the Google‚Äôs Cloud Vision API

-   [googleLanguageR](https://github.com/ropensci/googleLanguageR) - Access Speech to Text, Entity analysis and translation APIs from R

-   [googleCloudRunner](https://code.markedmondson.me/googleCloudRunner/) - Continuous Development and Integration with Cloud Run, Cloud Scheduler and Cloud Build

## AWS

La documentation officielle de Amazon Web Service dispose [d'un tutoriel](https://aws.amazon.com/fr/blogs/opensource/getting-started-with-r-on-amazon-web-services/) pour acc√©der aux donn√©es du service depuis R.

# API REST

Le plus simple selon moi pour requ√™ter une API REST avec R est d'utiliser l'excellent `{httr2}` de Hadley Wickham (que vous connaissez d√©j√† certainement si vous utilisez le Tidyverse). La documentation du package est disponible [ici](https://httr2.r-lib.org/)

La premi√®re chose √† faire est √©videmment de lire la documentation de l'API que l'on souhaite requ√™ter. Pour vous entra√Æner, vous pouvez utiliser [une des API disponibles sur data.gouv.fr](https://www.data.gouv.fr/dataservices). Pour cet exemple, j'utiliserai [l'API de Hub'eau pour la qualit√© de l'eau potable en France](https://hubeau.eaufrance.fr/page/api-qualite-eau-potable#console). \n

Une fois rendus sur le site, la documentation nous indique que la "Base URL" est hubeau.eaufrance.fr/api, puis nous indique les extensions √† ajouter selon ce que l'on souhaite requ√™ter (liste des communes ou r√©sultats), puis les parm√®tres de filtrage

On utilisera √©galement la librairie `{jsonlite}` pour convertir les donn√©es r√©cup√©r√©es au format JSON en un data.frame

::: callout-tip
## Astuce
Le package `{httr2}` n√©cessite l'installation du package `{curl}`, qui peut poser probl√®me si vous √™tes sur un r√©seau professionnel prot√©g√© par un pare-feu. Dans ce cas, contactez votre DSI, ou ... faites un partage de connexion depuis votre t√©l√©phone ü§´
:::

```{r hubeau}
library(httr2)
library(jsonlite)

base_url <- "http://hubeau.eaufrance.fr/api"

req <- request(base_url) |>  # pointer vers l'url de base
  req_url_path_append("/v1/qualite_eau_potable/resultats_dis") |> #extension
  req_url_query(code_departement = "85") |> # param√®tres de la requ√™te
  req_url_query(size = "50") |> 
  req_url_query(    
    fields = c('libelle_parametre','libelle_parametre_maj',
               'resultat_numerique', 'libelle_unite',
               'limite_qualite_parametre','reference_qualite_parametre',
               'nom_commune','date_prelevement',
               'conclusion_conformite_prelevement'), 
    .multi = "comma")

resp <- req_perform(req)

df_data <- resp |>
  resp_body_string() |>
  fromJSON()

df_data <- df_data$data

head(df_data)

```

```{r hubeau_2}
#| eval: true
#| echo: false

head(readr::read_csv("../../data/hubeau_data.csv"))
```


::: callout-tip
## Cacher les cl√©s API
Dans le cas o√π vous utilisez une API priv√©e n√©cessitant une cl√©, il est judicieux d'√©viter de stocker vos identifiants dans le script R. Pour cela, vous pouvez utiliser un fichier [.Rprofile](https://docs.posit.co/ide/user/ide/guide/environments/r/managing-r.html), stock√© sur votre machine locale, et qui se lance quand vous d√©marrez votre IDE.
Ainsi, vos identifiants sont pr√©sents dans votre environnement sans avoir √† les d√©clarer dans le script. Si vous h√©bergez votre code sur GitHub, vous pouvez inclure le .Rprofile dans le .gitignore pour √©viter de les inclure dans le repo, et utiliser la fonction [*Github Secrets*](https://docs.github.com/fr/actions/how-tos/write-workflows/choose-what-workflows-do/use-secrets) pour remplacer le .Rprofile.
:::



# Webscraping  

Si vous souhaitez r√©cup√©rer des donn√©es sur un site web qui ne propose pas d'API, l'une des solutions est le [*web scraping*](https://fr.wikipedia.org/wiki/Web_scraping). 


::: callout-warning
## Attention
Tous les sites n'autorisent pas le web scraping, et les donn√©es doivent √™tre *nativement* publiques, *tomb√©es* dans le domaine publique ou sous licence libre *si votre usage n'est pas commercial*. Renseignez-vous avant de scraper une page !
:::

Le principal package de web scraping en R est [`{rvest}`](https://rvest.tidyverse.org/index.html) (toujours d√©velopp√© par Hadley Wickham). 
Pour cet exemple, j'utiliserai aussi [`{polite}`](https://dmi3kno.github.io/polite/index.html), un package qui permet de suivre 3 r√®gles d'√©thique lors d'une session de scraping : *"Demander la permission, prendre doucement et ne jamais demander deux fois"*.
Ces r√®gles permettent d'√©viter de causer des probl√®mes en r√©coltant des donn√©es non autoris√©es ou en surchargeant le serveur de requ√™tes, et nous √©viterons d'√™tre bannis par les administrateurs du site scrap√©.

Dans cet exemple, nous allons r√©cup√©rer une table contenant la liste des communes de Vend√©e, sur [cette page wikip√©dia](https://fr.wikipedia.org/wiki/Liste_des_communes_de_la_Vend%C3%A9e)

```{r webscraping}

library(rvest)
library(polite)

url <- "https://fr.wikipedia.org/wiki/Liste_des_communes_de_la_Vend%C3%A9e"
session <- polite::bow(url) 
# Bow permet d'interroger le robots.txt et nous informe du r√©sultat
# il enregistre notamment le d√©lai minimum de requ√™tage autoris√© par le site

session

page <- polite::scrape(session)
# scrape() r√©cup√®re le contenu autoris√©

cities_table <- page |> 
  rvest::html_element("table.wikitable") |> 
  rvest::html_table()

# html_element r√©cup√®re le premier √©l√©ment de la classe "table.wikitable"
# pour tous les r√©cup√©rer sous forme de liste : html_elements()
# html_table() permet de convertir des donn√©es tabulaires en un dataframe

head(cities_table)

```

```{r webscraping_2}
#| eval: true
#| echo: false
head(readr::read_csv("../../data/cities_table"))
```

# OCR et PDF  

Dans le domaine de la sant√©, les compte-rendus m√©dicaux contiennent √©norm√©ment de donn√©es int√©ressantes, mais sont malheureusement difficilement exploitables car dans un format non-structur√© : un papier scann√© et stock√© en PDF.

L'OCR (Optical Character Recognition) est un syst√®me de *machine learning* permettant d'extraire le texte pr√©sent sur une image, parfait donc pour exploiter un grand nombre de comtpe-rendus scann√©s sans avoir √† le faire manuellement.

Dans cet exemple, nous utiliserons le mod√®le **Tesseract**, d√©velopp√© par Google et aujourd'hui Open Source. Nous utiliserons √©galement la librairie `doParallel` pour parall√©liser le traitement des images et augmenter la vitesse d'OCRisation

```{r ocr}
library(tidyverse)
library(tesseract)
library(doParallel)
library(pdftools)

# R√©cup√©rer le chemin de tous les fichiers PDF pr√©sents dans un dossier
list_pdf <- list.files(path = "path/to/folder", 
                       pattern = "\\.pdf$", 
                       all.files = TRUE, 
                       full.names = TRUE,
                       recursive = FALSE
                       )

ncores <- doParallel::detectCores(logical = FALSE) # compter les coeurs physiques 

# Cr√©er le cluster, en laissant 2 coeurs non utilis√©s
cl <- doParallel::makeCluster(ncores-2)   
# et pour arr√™ter : stopCluster(cl)


# Charger les librariries et les objets dans les clusters
## clusterEvalQ() : ex√©cute le code dans le cluster
clusterEvalQ(cl, {
  library(tidyverse);
  library(tesseract);
  library(pdftools)
  })

## clusterExport() charge l'objet dans le cluster
clusterExport(cl, c("list_pdf"))


# Cr√©er une fonction qui renvoie un dataframe en sortie (1 ligne par page)

myPdfConvert <- function(list_objet) { 
  list_objet %>% map_df(~ data.frame(doc_origin = .x, #.x = chaque √©l√©ment de listobjet
                                     texte = ocr(pdftools::pdf_convert(.x, dpi = 200)) # oc√©rise un pdf converti en png
                                     ))
}


# Appliquer la fonction myPdfConvert √† chaque √©l√©ment de list_pdf
res <- parLapply(cl, list_pdf, myPdfConvert)
# Le r√©sultat est obtenu sous forme de liste (1 √©l√©ment par cluster) => les recombiner
res <- do.call("rbind", res)

# Arr√™ter le cluster
stopCluster(cl)


# Pour r√©cup√©rer num√©ro de page et nom du doc √† chaque ligne (1 ligne = 1 page)
resr <- res %>% 
  group_by(doc_origin) %>% 
  mutate(doc_page = 1:n(), doc_nom = str_extract_all(doc_origin, "[[:alnum:]_ ]*\\.pdf")) %>%
  ungroup()


```


Pour visualiser l'impact de la parall√©lisation : 

```{r ocr2}
#| echo: false
#| eval: true

library(ggplot2)

plot <- data.frame(
  time_s = c(16,34,49,79,140,28,95,148,340,445),
  cluster = c(rep("Parall√©lis√©",5), rep("Normal",5)),
  nombre_pages = c(24,60,108,216,300,24,60,108,216,300)
  )

ggplot(plot,
       aes(x = nombre_pages, 
           y = time_s, 
           group = cluster, 
           color = cluster)
       ) +
  geom_line() +
  geom_point() +
  ylab("Temps en secondes") +
  scale_x_continuous("Nombre de pages", limits = c(0,300)) +
  ggtitle("Comparaison du temps d'OCR selon le mode de calcul")

```





# JSON et XML  

Les formats **XML** (eXtensible Markup Language) et **JSON** (JavaScript Object Notation) sont largement utilis√©s pour le stockage et l‚Äô√©change de donn√©es, notamment dans les **API REST**, les **flux RSS**, ou le stockage de donn√©es hi√©rarchiques .

Pour lire les fichiers XML, on utilisera`{xml2}` et pour JSON :`{jsonlite}` (il en existe de nombreuses autres, mais ce sont les plus connues)

## XML  

Supposons que nous ayons un fichier "livres.xml" ressemblant √† cela :

```{r, xml_1}
#| echo: TRUE
#| eval: TRUE
library(xml2)

xml_doc <- as_xml_document(
"<livres>
  <livre>
    <titre>R pour les d√©butants</titre>
    <auteur>Jean Martin</auteur>
    <annee>2020</annee>
  </livre>
  <livre>
    <titre>Analyse de donn√©es avec R</titre>
    <auteur>Marie Dupont</auteur>
    <annee>2021</annee>
  </livre>
</livres>"
)

```

Pour lire le fichier .xml, on utilisera

```{r, xml_2}
#| echo: TRUE
#| eval: FALSE

xml_doc <- xml2::read_xml("livres.xml")
```

Ensuite, on peut extraire les titres avec

```{r xml_3}
#| echo: TRUE
#| eval: TRUE

titres_xml <- xml_find_all(xml_doc, ".//livre/titre")
print(titres_xml)
```

Les fonctions `xml_find_*()` permettent d'utiliser des expression **xpath**, similaire √† du **REGEX** mais pour les architectures en arbre et renvoient des objets de classe `xml_nodeset`. Pour r√©cup√©rer un vecteur `character`, on rajoute simplement `xml_text()`

```{r xml_4}
#| echo: TRUE
#| eval: TRUE
titres <- xml_text(titres_xml)
print(titres)
```


## JSON  

Pour lire du JSON, on utilisera simplement `fromJSON()`, pour convertir une cha√Æne de texte, ou `read_json()` pour lire un fichier .json. Les deux fonctions renvoient directement un `data.frame`

```{r json_1}
#| echo: TRUE
#| eval: TRUE
#| results: "asis"
library(jsonlite)

livres_json <- fromJSON(
'
[
  {
    "titre": "R pour les d√©butants",
    "auteur": "Jean Dupont",
    "annee": 2020
  },
  {
    "titre": "Analyse de donn√©es avec R",
    "auteur": "Marie Curie",
    "annee": 2021
  }
]
')

print(livres_json)
```

On citera aussi la possibilit√© de convertir un objet R en JSON avec la fonction `toJSON()`. 



# DICOM  

Sans rentrer dans les d√©tails ici, le format DICOM (Digital Imaging and Communications in Medicine) est un standard pour l'imagerie m√©dicale. le package `{Espadon}` [d√©velopp√© par le CNRS](https://espadon.cnrs.fr/) permet d'interagir avec ce format

# GPX  

GPX est un format pour les donn√©es g√©ospatiales, qui permet par exemple de cr√©er un itin√©raire sur une carte. [Cet article](https://www.appsilon.com/post/r-gpx-files) explique tr√®s bien comment lire ce standard, souvent stock√© au format XML.






















